@inproceedings{A.MohamedBERTELMo2019,
  title = {{{BERT}}, {{ELMo}}, {{USE}} and {{InferSent Sentence Encoders}}: {{The Panacea}} for {{Research-Paper Recommendation}}?},
  shorttitle = {{{BERT}}, {{ELMo}}, {{USE}} and {{InferSent Sentence Encoders}}},
  author = {A. Mohamed, Hebatallah and Sansonetti, Giuseppe and Gasparetti, Fabio and Micarelli, Alessandro and Beel, Joeran},
  year = {2019},
  month = sep,
  abstract = {Content-based approaches to research paper recommendation are important when user feedback is sparse or not available. The task of content-based matching is challenging, mainly due to the problem of determining the semantic similarity of texts. Nowadays, there exist many sentence embedding models that learn deep semantic representations by being trained on huge corpora, aiming to provide transfer learning to a wide variety of natural language processing tasks. In this work, we present a comparative evaluation among five well-known pre-trained sentence encoders deployed in the pipeline of title-based research paper recommendation. The experimented encoders are USE, BERT, InferSent, ELMo, and SciBERT. For our study, we propose a methodology for evaluating such models in reranking BM25-based recommendations. The experimental results show that the sole consideration of semantic information from these encoders does not lead to improved recommendation performance over the traditional BM25 technique, while their integration enables the retrieval of a set of relevant papers that may not be retrieved by the BM25 ranking function. KEYWORDS Pre-trained sentence embeddings, semantic similarity, reranking, research paper recommendation INTRODUCTION Sentence encoders such as Google's BERT and USE, Facebook's InferSent, and AllenAI's SciBERT and ELMo, have received significant attention in recent years. These pre-trained machine learning models can encode a sentence into deep contextualized embeddings. They have been reported to outperform previous state-of-the-art approaches such as traditional word embeddings, for many natural language processing tasks [2, 3, 5, 7, 8]. Such tasks also include the calculation of semantic similarity and relatedness, which is key in developing effective research-paper recommender systems. If sentence encoders performed for calculating relatedness of research articles as good as for other tasks, this would mean a great advancement for research-paper recommender systems. There are some work on using document embeddings for ranking research papers based on semantic relatedness (e.g., [4]), but-as far as we know-no work on exploiting pre-trained sentence embeddings for the same task. Existing work on sentence encoders focused on different domains such as social media posts [10], news [6, 10], and web pages [6]. Our goal is to find how well some of the most common sentence encoders perform for calculating document relatedness in the scenario of a research-paper recommender system. To the best of our knowledge, we are the first to conduct such an evaluation in the field of research paper recommendations.}
}

@inproceedings{AbdallaElephantRoom2023,
  title = {The {{Elephant}} in the {{Room}}: {{Analyzing}} the {{Presence}} of {{Big Tech}} in {{Natural Language Processing Research}}},
  shorttitle = {The {{Elephant}} in the {{Room}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Abdalla, Mohamed and Wahle, Jan Philip and Lima Ruas, Terry and N{\'e}v{\'e}ol, Aur{\'e}lie and Ducel, Fanny and Mohammad, Saif and Fort, Karen},
  year = {2023},
  month = jul,
  pages = {13141--13160},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.734},
  url = {https://aclanthology.org/2023.acl-long.734},
  urldate = {2023-08-28},
  abstract = {Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180\% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are significant and fast-growing. This work calls for increased transparency of industry influence in the field.}
}

@article{AdomaviciusNextGeneration2005,
  title = {Toward the next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions},
  shorttitle = {Toward the next Generation of Recommender Systems},
  author = {Adomavicius, G. and Tuzhilin, A.},
  year = {2005},
  month = jun,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {17},
  number = {6},
  pages = {734--749},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2005.99},
  abstract = {This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.}
}

@inproceedings{AgarwalResearchPaper2005,
  title = {Research {{Paper Recommender Systems}}: {{A Subspace Clustering Approach}}},
  shorttitle = {Research {{Paper Recommender Systems}}},
  booktitle = {Advances in {{Web-Age Information Management}}},
  author = {Agarwal, Nitin and Haque, Ehtesham and Liu, Huan and Parsons, Lance},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Fan, Wenfei and Wu, Zhaohui and Yang, Jun},
  year = {2005},
  volume = {3739},
  pages = {475--491},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11563952_42},
  url = {http://link.springer.com/10.1007/11563952_42},
  urldate = {2023-03-07},
  abstract = {Researchers from the same lab often spend a considerable amount of time searching for published articles relevant to their current project. Despite having similar interests, they conduct independent, time consuming searches. While they may share the results afterwards, they are unable to leverage previous search results during the search process. We propose a research paper recommender system that avoids such time consuming searches by augmenting existing search engines with recommendations based on previous searches performed by others in the lab. Most existing recommender systems were developed for commercial domains with millions of users. The research paper domain has relatively few users compared to the large number of online research papers. The two major challenges with this type of data are the large number of dimensions and the sparseness of the data. The novel contribution of the paper is a scalable subspace clustering algorithm (SCuBA) that tackles these problems. Both synthetic and benchmark datasets are used to evaluate the clustering algorithm and to demonstrate that it performs better than the traditional collaborative filtering approaches when recommending research papers.},
  isbn = {978-3-540-29227-2 978-3-540-32087-6}
}

@inproceedings{AkkalyoncuYilmazApplyingBERT2019,
  title = {Applying {{BERT}} to {{Document Retrieval}} with {{Birch}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}}): {{System Demonstrations}}},
  author = {Akkalyoncu Yilmaz, Zeynep and Wang, Shengjin and Yang, Wei and Zhang, Haotian and Lin, Jimmy},
  year = {2019},
  month = nov,
  pages = {19--24},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-3004},
  url = {https://aclanthology.org/D19-3004},
  urldate = {2023-04-25},
  abstract = {We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve state-of-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.}
}

@misc{AlammarIllustratedBERT2018,
  title = {The {{Illustrated BERT}}, {{ELMo}}, and Co. ({{How NLP Cracked Transfer Learning}})},
  author = {Alammar, Jay},
  year = {2018},
  month = dec,
  url = {https://jalammar.github.io/illustrated-bert/},
  urldate = {2023-02-01},
  abstract = {Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish 2021 Update: I created this brief and highly accessible video intro to BERT The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It's been referred to as NLP's ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).}
}

@misc{AlammarIllustratedTransformer2018,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  year = {2018},
  month = jun,
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2023-02-01},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Japanese, Korean, Persian, Russian, Spanish, Vietnamese Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention \textendash{} a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \textendash{} a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I've created a ``Narrated Transformer'' video which is a gentler approach to the topic: A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.}
}

@misc{AlammarIllustratedWord2vec2019,
  title = {The {{Illustrated Word2vec}}},
  author = {Alammar, Jay},
  year = {2019},
  month = mar,
  url = {https://jalammar.github.io/illustrated-word2vec/},
  urldate = {2023-02-01},
  abstract = {Discussions: Hacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments) Translations: Chinese (Simplified), French, Korean, Portuguese, Russian         ``There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote   bush or the pattern of its leaves.    We try to copy these patterns in our lives and our society,   seeking the rhythms, the dances, the forms that comfort.   Yet, it is possible to see peril in the finding of   ultimate perfection. It is clear that the ultimate   pattern contains it own fixity. In such   perfection, all things move toward death.''   \textasciitilde{} Dune (1965) I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you've ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you've benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2). Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines. In this post, we'll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let's start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?}
}

@misc{AlammarVisualGuide2019,
  title = {A {{Visual Guide}} to {{Using BERT}} for the {{First Time}}},
  author = {Alammar, Jay},
  year = {2019},
  month = nov,
  url = {https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/},
  urldate = {2023-02-01},
  abstract = {Translations: Chinese, Korean, Russian       Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents ``the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search''. This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved. Alongside this post, I've prepared a notebook. You can see it here the notebook or run it on colab.}
}

@article{AliGraphbasedTaxonomy2020,
  title = {A Graph-Based Taxonomy of Citation Recommendation Models},
  author = {Ali, Zafar and Qi, Guilin and Kefalas, Pavlos and Abro, Waheed Ahmad and Ali, Bahadar},
  year = {2020},
  month = oct,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {7},
  pages = {5217--5260},
  issn = {0269-2821},
  doi = {10.1007/s10462-020-09819-4},
  url = {https://doi.org/10.1007/s10462-020-09819-4},
  urldate = {2023-07-22},
  abstract = {Recommender systems have been used since the beginning of the Web to assist users with personalized suggestions related to past preferences for items or products including books, movies, images, research papers and web pages. The availability of millions research articles on various digital libraries makes it difficult for a researcher to find relevant articles to his/er research. During the last years, a lot of research have been conducted through models and algorithms that personalize papers recommendations. With this survey, we explore the state-of-the-art citation recommendation models which we categorize using the following seven criteria: platform used, data factors/features, data representation methods, methodologies and models, recommendation types, problems addressed, and personalization. In addition, we present a novel k-partite graph-based taxonomy that examines the relationships among surveyed algorithms and corresponding k-partite graphs used. Moreover, we present (a) domain's popular issues, (b) adopted metrics, and (c) commonly used datasets. Finally, we provide some research trends and future directions.}
}

@article{AroraSimpleToughtobeat2017,
  title = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  year = {2017},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013).},
  langid = {english}
}

@misc{BahdanauNeuralMachine2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2023-07-21},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv}
}

@misc{BaiScientificPaper2020,
  title = {Scientific {{Paper Recommendation}}: {{A Survey}}},
  shorttitle = {Scientific {{Paper Recommendation}}},
  author = {Bai, Xiaomei and Wang, Mengyang and Lee, Ivan and Yang, Zhuo and Kong, Xiangjie and Xia, Feng},
  year = {2020},
  month = aug,
  number = {arXiv:2008.13538},
  eprint = {2008.13538},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2008.13538},
  url = {http://arxiv.org/abs/2008.13538},
  urldate = {2023-03-23},
  abstract = {Globally, recommendation services have become important due to the fact that they support e-commerce applications and different research communities. Recommender systems have a large number of applications in many fields including economic, education, and scientific research. Different empirical studies have shown that recommender systems are more effective and reliable than keyword-based search engines for extracting useful knowledge from massive amounts of data. The problem of recommending similar scientific articles in scientific community is called scientific paper recommendation. Scientific paper recommendation aims to recommend new articles or classical articles that match researchers' interests. It has become an attractive area of study since the number of scholarly papers increases exponentially. In this survey, we first introduce the importance and advantages of paper recommender systems. Second, we review the recommendation algorithms and methods, such as Content-Based methods, Collaborative Filtering methods, Graph-Based methods and Hybrid methods. Then, we introduce the evaluation methods of different recommender systems. Finally, we summarize open issues in the paper recommender systems, including cold start, sparsity, scalability, privacy, serendipity and unified scholarly data standards. The purpose of this survey is to provide comprehensive reviews on scholarly paper recommendation.},
  archiveprefix = {arxiv}
}

@misc{BarlaUltimateGuide2022,
  title = {The {{Ultimate Guide}} to {{Word Embeddings}}},
  author = {Barla, Nilesh},
  year = {2022},
  month = jul,
  journal = {neptune.ai},
  url = {https://neptune.ai/blog/word-embeddings-guide},
  urldate = {2023-07-28},
  abstract = {Word embeddings is one of the most used techniques in natural language processing (NLP). It's often said that the performance and ability of SOTA models wouldn't have been possible without word embeddings. It's precisely because of word embeddings that language models like RNNs, LSTMs, ELMo, BERT, AlBERT, GPT-2 to the most recent GPT-3 have evolved\ldots},
  langid = {american}
}

@incollection{BeelComparisonOffline2015,
  title = {A {{Comparison}} of {{Offline Evaluations}}, {{Online Evaluations}}, and {{User Studies}} in the {{Context}} of {{Research-Paper Recommender Systems}}},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Beel, Joeran and Langer, Stefan},
  editor = {Kapidakis, Sarantos and Mazurek, Cezary and Werla, Marcin},
  year = {2015},
  volume = {9316},
  pages = {153--168},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24592-8_12},
  url = {http://link.springer.com/10.1007/978-3-319-24592-8_12},
  urldate = {2023-07-19},
  abstract = {The evaluation of recommender systems is key to the successful application of recommender systems in practice. However, recommender-systems evaluation has received too little attention in the recommender-system community, in particular in the community of research-paper recommender systems. In this paper, we examine and discuss the appropriateness of different evaluation methods, i.e. offline evaluations, online evaluations, and user studies, in the context of research-paper recommender systems. We implemented different content-based filtering approaches in the research-paper recommender system of Docear. The approaches differed by the features to utilize (terms or citations), by user model size, whether stop-words were removed, and several other factors. The evaluations show that results from offline evaluations sometimes contradict results from online evaluations and user studies. We discuss potential reasons for the non-predictive power of offline evaluations, and discuss whether results of offline evaluations might have some inherent value. In the latter case, results of offline evaluations were worth to be published, even if they contradict results of user studies and online evaluations. However, although offline evaluations theoretically might have some inherent value, we conclude that in practice, offline evaluations are probably not suitable to evaluate recommender systems, particularly in the domain of research paper recommendations. We further analyze and discuss the appropriateness of several online evaluation metrics such as click-through rate, linkthrough rate, and cite-through rate.},
  isbn = {978-3-319-24591-1 978-3-319-24592-8},
  langid = {english}
}

@article{BeelResearchpaperRecommender2016,
  title = {Research-Paper Recommender Systems: A Literature Survey},
  shorttitle = {Research-Paper Recommender Systems},
  author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
  year = {2016},
  month = nov,
  journal = {International Journal on Digital Libraries},
  volume = {17},
  number = {4},
  pages = {305--338},
  issn = {1432-1300},
  doi = {10.1007/s00799-015-0156-0},
  url = {https://doi.org/10.1007/s00799-015-0156-0},
  urldate = {2023-02-27},
  abstract = {In the last 16~years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55~\%). Collaborative filtering was applied by only 18~\% of the reviewed approaches, and graph-based recommendations by 16~\%. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81~\%) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10~\% of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73~\% of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.},
  langid = {english}
}

@misc{BeltagyLongformerLongDocument2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  month = dec,
  number = {arXiv:2004.05150},
  eprint = {2004.05150},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.05150},
  url = {http://arxiv.org/abs/2004.05150},
  urldate = {2023-05-01},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arxiv}
}

@misc{BeltagySciBERTPretrained2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  month = sep,
  number = {arXiv:1903.10676},
  eprint = {1903.10676},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.10676},
  url = {http://arxiv.org/abs/1903.10676},
  urldate = {2023-03-20},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  archiveprefix = {arxiv}
}

@misc{BhagavatulaContentBasedCitation2018,
  title = {Content-{{Based Citation Recommendation}}},
  author = {Bhagavatula, Chandra and Feldman, Sergey and Power, Russell and Ammar, Waleed},
  year = {2018},
  month = feb,
  number = {arXiv:1802.08301},
  eprint = {1802.08301},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.08301},
  url = {http://arxiv.org/abs/1802.08301},
  urldate = {2023-04-03},
  abstract = {We present a content-based method for recommending citations in an academic paper draft. We embed a given query document into a vector space, then use its nearest neighbors as candidates, and rerank the candidates using a discriminative model trained to distinguish between observed and unobserved citations. Unlike previous work, our method does not require metadata such as author names which can be missing, e.g., during the peer review process. Without using metadata, our method outperforms the best reported results on PubMed and DBLP datasets with relative improvements of over 18\% in F1@20 and over 22\% in MRR. We show empirically that, although adding metadata improves the performance on standard metrics, it favors self-citations which are less useful in a citation recommendation setup. We release an online portal (http://labs.semanticscholar.org/citeomatic/) for citation recommendation based on our method, and a new dataset OpenCorpus of 7 million research articles to facilitate future research on this task.},
  archiveprefix = {arxiv}
}

@misc{BojanowskiEnrichingWord2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  number = {arXiv:1607.04606},
  eprint = {1607.04606},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1607.04606},
  url = {http://arxiv.org/abs/1607.04606},
  urldate = {2023-03-20},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arxiv}
}

@misc{BornmannGrowthRates2014,
  title = {Growth Rates of Modern Science: {{A}} Bibliometric Analysis Based on the Number of Publications and Cited References},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Mutz, Ruediger},
  year = {2014},
  month = may,
  number = {arXiv:1402.4578},
  eprint = {1402.4578},
  primaryclass = {physics, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1402.4578},
  url = {http://arxiv.org/abs/1402.4578},
  urldate = {2023-08-20},
  abstract = {Many studies in information science have looked at the growth of science. In this study, we re-examine the question of the growth of science. To do this we (i) use current data up to publication year 2012 and (ii) analyse it across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data are analysed with an advanced statistical technique - segmented regression analysis - which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (1) The number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (2) the number of cited references in the publications of the source items per cited reference year. We have looked at the rate at which science has grown since the mid-1600s. In our analysis of cited references we identified three growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1\% up to the middle of the 18th century, to 2 to 3\% up to the period between the two world wars and 8 to 9\% to 2012.},
  archiveprefix = {arxiv}
}

@article{BreitingerAcademicLiterature2023,
  title = {Academic {{Literature Recommendation}} Using {{Semantic Feature Analysis}}},
  author = {Breitinger, Corinna},
  year = {2023},
  langid = {english}
}

@misc{BreitingerThesisProject2022,
  title = {Thesis {{Project Description}} - {{LR2}}: {{Implementing}} a {{Hybrid Recommendation Approach}}},
  author = {Breitinger, Corinna},
  year = {2022}
}

@misc{BriggsSemanticSearch2020,
  title = {Semantic {{Search}}: {{Measuring Meaning From Jaccard}} to {{Bert}}},
  shorttitle = {Semantic {{Search}}},
  author = {Briggs, James},
  year = {2020},
  journal = {Pinecone},
  url = {https://www.pinecone.io/learn/semantic-search/},
  urldate = {2023-04-24},
  abstract = {Supercharge search with these stellar technologies.},
  langid = {english}
}

@misc{BrownLanguageModels2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-06-25},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  annotation = {4879 citations (Semantic Scholar/arXiv) [2022-07-26]}
}

@misc{BrownRankBM25Collection2020,
  title = {Rank-{{BM25}}: {{A}} Collection of {{BM25}} Algorithms in Python},
  author = {Brown, Dorian},
  year = {2020},
  doi = {10.5281/zenodo.4520057},
  url = {https://doi.org/10.5281/zenodo.4520057},
  howpublished = {Zenodo}
}

@article{BurkeHybridRecommender2002,
  title = {Hybrid {{Recommender Systems}}: {{Survey}} and {{Experiments}}},
  shorttitle = {Hybrid {{Recommender Systems}}},
  author = {Burke, Robin},
  year = {2002},
  month = nov,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {12},
  number = {4},
  pages = {331--370},
  issn = {1573-1391},
  doi = {10.1023/A:1021240730564},
  url = {https://doi.org/10.1023/A:1021240730564},
  urldate = {2023-04-13},
  abstract = {Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. A variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. To improve performance, these methods have sometimes been combined in hybrid recommenders. This paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, EntreeC, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. Further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering.},
  langid = {english}
}

@inproceedings{BurkeHybridWeb2007,
  title = {Hybrid {{Web Recommender Systems}}},
  booktitle = {The {{Adaptive Web}}},
  author = {Burke, Robin},
  editor = {Brusilovsky, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
  year = {2007},
  volume = {4321},
  pages = {377--408},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-72079-9_12},
  url = {http://link.springer.com/10.1007/978-3-540-72079-9_12},
  urldate = {2023-08-16},
  abstract = {Adaptive web sites may offer automated recommendations generated through any number of well-studied techniques including collaborative, content-based and knowledge-based recommendation. Each of these techniques has its own strengths and weaknesses. In search of better performance, researchers have combined recommendation techniques to build hybrid recommender systems. This chapter surveys the space of two-part hybrid recommender systems, comparing four different recommendation techniques and seven different hybridization strategies. Implementations of 41 hybrids including some novel combinations are examined and compared. The study finds that cascade and augmented hybrids work well, especially when combining two components of differing strengths.},
  isbn = {978-3-540-72078-2},
  langid = {english}
}

@misc{CerUniversalSentence2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  number = {arXiv:1803.11175},
  eprint = {1803.11175},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.11175},
  url = {http://arxiv.org/abs/1803.11175},
  urldate = {2023-09-15},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arxiv}
}

@misc{chiangTypesHybrid2021,
  title = {7 {{Types}} of {{Hybrid Recommendation System}}},
  author = {{chiang}, Jeffery},
  year = {2021},
  month = jun,
  journal = {Analytics Vidhya},
  url = {https://medium.com/analytics-vidhya/7-types-of-hybrid-recommendation-system-3e4f78266ad8},
  urldate = {2023-03-20},
  abstract = {Combining multiple recommendation system techniques to effectively predict the users' habit},
  langid = {english}
}

@misc{CohanSPECTERDocumentlevel2020,
  title = {{{SPECTER}}: {{Document-level Representation Learning}} Using {{Citation-informed Transformers}}},
  shorttitle = {{{SPECTER}}},
  author = {Cohan, Arman and Feldman, Sergey and Beltagy, Iz and Downey, Doug and Weld, Daniel S.},
  year = {2020},
  month = may,
  number = {arXiv:2004.07180},
  eprint = {2004.07180},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.07180},
  url = {http://arxiv.org/abs/2004.07180},
  urldate = {2023-03-08},
  abstract = {Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.},
  archiveprefix = {arxiv}
}

@misc{ConellyPracticalBM252018,
  title = {Practical {{BM25}} - {{Part}} 2: {{The BM25 Algorithm}} and Its {{Variables}}},
  shorttitle = {Practical {{BM25}} - {{Part}} 2},
  author = {Conelly, Shane},
  year = {2018},
  month = apr,
  journal = {Elastic Blog},
  url = {https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables},
  urldate = {2023-04-19},
  abstract = {BM25 is the default similarity ranking (relevancy) algorithm in Elasticsearch. Learn more about how it works by digging into the equation and exploring the concepts behind its variables.}
}

@misc{ConneauSupervisedLearning2018,
  title = {Supervised {{Learning}} of {{Universal Sentence Representations}} from {{Natural Language Inference Data}}},
  author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  year = {2018},
  month = jul,
  number = {arXiv:1705.02364},
  eprint = {1705.02364},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.02364},
  url = {http://arxiv.org/abs/1705.02364},
  urldate = {2023-09-15},
  abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
  archiveprefix = {arxiv}
}

@inproceedings{CovingtonDeepNeural2016,
  title = {Deep {{Neural Networks}} for {{YouTube Recommendations}}},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Covington, Paul and Adams, Jay and Sargin, Emre},
  year = {2016},
  address = {{New York, NY, USA}}
}

@inproceedings{DavidsonYouTubeVideo2010,
  title = {The {{YouTube}} Video Recommendation System},
  booktitle = {Proceedings of the Fourth {{ACM}} Conference on {{Recommender}} Systems},
  author = {Davidson, James and Liebald, Benjamin and Liu, Junning and Nandy, Palash and Van Vleet, Taylor and Gargi, Ullas and Gupta, Sujoy and He, Yu and Lambert, Mike and Livingston, Blake and Sampath, Dasarathi},
  year = {2010},
  month = sep,
  pages = {293--296},
  publisher = {{ACM}},
  address = {{Barcelona Spain}},
  doi = {10.1145/1864708.1864770},
  url = {https://dl.acm.org/doi/10.1145/1864708.1864770},
  urldate = {2023-07-19},
  abstract = {We discuss the video recommendation system in use at YouTube, the world's most popular online video community. The system recommends personalized sets of videos to users based on their activity on the site. We discuss some of the unique challenges that the system faces and how we address them. In addition, we provide details on the experimentation and evaluation framework used to test and tune new algorithms. We also present some of the findings from these experiments.},
  isbn = {978-1-60558-906-0},
  langid = {english}
}

@inproceedings{DeSouzaPereiraMoreiraTransformers4RecBridging2021,
  title = {{{Transformers4Rec}}: {{Bridging}} the {{Gap}} between {{NLP}} and {{Sequential}} / {{Session-Based Recommendation}}},
  shorttitle = {{{Transformers4Rec}}},
  booktitle = {Fifteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {De Souza Pereira Moreira, Gabriel and Rabhi, Sara and Lee, Jeong Min and Ak, Ronay and Oldridge, Even},
  year = {2021},
  month = sep,
  pages = {143--153},
  publisher = {{ACM}},
  address = {{Amsterdam Netherlands}},
  doi = {10.1145/3460231.3474255},
  url = {https://dl.acm.org/doi/10.1145/3460231.3474255},
  urldate = {2023-07-21},
  abstract = {Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pretraining techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation. But with a thousandfold more research going on in NLP, the application of transformers for recommendation understandably lags behind. To remedy this we introduce Transformers4Rec, an open-source library built upon HuggingFace's Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation. Like its core dependency, Transformers4Rec is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.},
  isbn = {978-1-4503-8458-2},
  langid = {english}
}

@article{DevlinBERTPretraining2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-02-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  annotation = {9998 citations (Semantic Scholar/arXiv) [2022-07-26]}
}

@misc{DeyCollaborativeFiltering2021,
  title = {Collaborative {{Filtering Vs Content-Based Filtering}} for {{Recommender Systems}}},
  author = {Dey, Victor},
  year = {2021},
  month = aug,
  journal = {Analytics India Magazine},
  url = {https://analyticsindiamag.com/collaborative-filtering-vs-content-based-filtering-for-recommender-systems/},
  urldate = {2023-02-27},
  abstract = {A Recommender system predict whether a particular user would prefer an item or not based on the user's profile and its information.},
  langid = {american}
}

@book{FalkPracticalRecommender2019,
  title = {Practical {{Recommender Systems}}},
  author = {Falk, Kim},
  year = {2019},
  month = feb,
  edition = {1st edition},
  publisher = {{Manning}},
  address = {{Shelter Island, NY}},
  abstract = {SummaryOnline recommender systems help users find movies, jobs, restaurants-even romance! There's an art in combining statistics, demographics, and query terms to achieve results that will delight them. Learn to build a recommender system the right way: it can make or break your application!Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyRecommender systems are everywhere, helping you find everything from movies to jobs, restaurants to hospitals, even romance. Using behavioral and demographic data, these systems make predictions about what users will be most interested in at a particular time, resulting in high-quality, ordered, personalized suggestions. Recommender systems are practically a necessity for keeping your site content current, useful, and interesting to your visitors.About the BookPractical Recommender Systems explains how recommender systems work and shows how to create and apply them for your site. After covering the basics, you'll see how to collect user data and produce personalized recommendations. You'll learn how to use the most popular recommendation algorithms and see examples of them in action on sites like Amazon and Netflix. Finally, the book covers scaling problems and other issues you'll encounter as your site grows.What's insideHow to collect and understand user behaviorCollaborative and content-based filteringMachine learning algorithms Real-world examples in PythonAbout the ReaderReaders need intermediate programming and database skills.About the AuthorKim Falk is an experienced data scientist who works daily with machine learning and recommender systems.Table of ContentsPART 1 - GETTING READY FOR RECOMMENDER SYSTEMSWhat is a recommender? User behavior and how to collect it Monitoring the system Ratings and how to calculate themNon-personalized recommendationsThe user (and content) who came in from the coldPART 2 - RECOMMENDER ALGORITHMSFinding similarities among users and among contentCollaborative filtering in the neighborhoodEvaluating and testing your recommenderContent-based filteringFinding hidden genres with matrix factorizationTaking the best of all algorithms: implementing hybrid recommendersRanking and learning to rankFuture of recommender systems},
  isbn = {978-1-61729-270-5},
  langid = {english}
}

@inproceedings{FarberHybridCiteHybrid2020,
  title = {{{HybridCite}}: {{A Hybrid Model}} for {{Context-Aware Citation Recommendation}}},
  shorttitle = {{{HybridCite}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} in 2020},
  author = {F{\"a}rber, Michael and Sampath, Ashwath},
  year = {2020},
  month = aug,
  eprint = {2002.06406},
  primaryclass = {cs},
  pages = {117--126},
  doi = {10.1145/3383583.3398534},
  url = {http://arxiv.org/abs/2002.06406},
  urldate = {2023-03-28},
  abstract = {Citation recommendation systems aim to recommend citations for either a complete paper or a small portion of text called a citation context. The process of recommending citations for citation contexts is called local citation recommendation and is the focus of this paper. Firstly, we develop citation recommendation approaches based on embeddings, topic modeling, and information retrieval techniques. We combine, for the first time to the best of our knowledge, the best-performing algorithms into a semi-genetic hybrid recommender system for citation recommendation. We evaluate the single approaches and the hybrid approach offline based on several data sets, such as the Microsoft Academic Graph (MAG) and the MAG in combination with arXiv and ACL. We further conduct a user study for evaluating our approaches online. Our evaluation results show that a hybrid model containing embedding and information retrieval-based components outperforms its individual components and further algorithms by a large margin.},
  archiveprefix = {arxiv}
}

@inproceedings{GilesCiteSeerAutomatic1998,
  title = {{{CiteSeer}}: An Automatic Citation Indexing System},
  shorttitle = {{{CiteSeer}}},
  booktitle = {Proceedings of the Third {{ACM}} Conference on {{Digital}} Libraries  - {{DL}} '98},
  author = {Giles, C. Lee and Bollacker, Kurt D. and Lawrence, Steve},
  year = {1998},
  pages = {89--98},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania, United States}},
  doi = {10.1145/276675.276685},
  url = {http://portal.acm.org/citation.cfm?doid=276675.276685},
  urldate = {2023-07-19},
  abstract = {We present CiteSeer: an autonomous citation indexing system which indexes academic literature in electronic format (e.g. Postscript files on the Web). CiteSeer understands how to parse citations, identify citations to the same paper in different formats, and identify the context of citations in the body of articles. CiteSeer provides most of the advantages of traditional (manually constructed) citation indexes (e.g. the ISI citation indexes), including: literature retrieval by following citation links (e.g. by providing a list of papers that cite a given paper), the evaluation and ranking of papers, authors, journals, etc. based on the number of citations, and the identification of research trends. CiteSeer has many advantages over traditional citation indexes, including the ability to create more up-to-date databases which are not limited to a preselected set of journals or restricted by journal publication delays, completely autonomous operation with a corresponding reduction in cost, and powerful interactive browsing of the literature using the context of citations. Given a particular paper of interest, CiteSeer can display the context of how the paper is cited in subsequent publications. This context may contain a brief summary of the paper, another author's response to the paper, or subsequent work which builds upon the original article. CiteSeer allows the location of papers by keyword search or by citation links. Papers related to a given paper can be located using common citation information or word vector similarity. CiteSeer will soon be available for public use.},
  isbn = {978-0-89791-965-4},
  langid = {english}
}

@book{GippCitationbasedPlagiarism2014,
  title = {Citation-Based {{Plagiarism Detection}}: {{Detecting Disguised}} and {{Cross-language Plagiarism}} Using {{Citation Pattern Analysis}}},
  shorttitle = {Citation-Based {{Plagiarism Detection}}},
  author = {Gipp, Bela},
  year = {2014},
  publisher = {{Springer Fachmedien}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-06394-8},
  url = {https://link.springer.com/10.1007/978-3-658-06394-8},
  urldate = {2023-03-07},
  isbn = {978-3-658-06393-1 978-3-658-06394-8},
  langid = {english}
}

@article{GippCitationbasedPlagiarism2014a,
  title = {Citation-Based Plagiarism Detection : Practicability on a Large-Scale Scientific Corpus},
  shorttitle = {Citation-Based Plagiarism Detection},
  author = {Gipp, Bela and Meuschke, Norman and Breitinger, Corinna},
  year = {2014},
  url = {http://kops.uni-konstanz.de/handle/123456789/30291},
  urldate = {2023-04-12},
  abstract = {The automated detection of plagiarism is an information retrieval task of increasing importance as the volume of readily accessible information on the web expands. A major shortcoming of current automated plagiarism detection approaches is their dependence on high character-based similarity. As a result, heavily disguised plagiarism forms, such as paraphrases, translated plagiarism, or structural and idea plagiarism, remain undetected. A recently proposed language-independent approach to plagiarism detection, Citation-based Plagiarism Detection (CbPD), allows the detection of semantic similarity even in the absence of text overlap by analyzing the citation placement in a document's full text to determine similarity. This article evaluates the performance of CbPD in detecting plagiarism with various degrees of disguise in a collection of 185,000 biomedical articles. We benchmark CbPD against two character-based detection approaches using a ground truth approximated in a user study. Our evaluation shows that the citation-based approach achieves superior ranking performance for heavily disguised plagiarism forms. Additionally, we demonstrate CbPD to be computationally more efficient than character-based approaches. Finally, upon combining the citation-based with the traditional character-based document similarity visualization methods in a hybrid detection prototype, we observe a reduction in the required user effort for document verification.},
  langid = {english}
}

@article{GippCitationProximity2009,
  title = {Citation {{Proximity Analysis}} ({{CPA}}) \textendash{} {{A}} New Approach for Identifying Related Work Based on {{Co-Citation Analysis}}},
  author = {Gipp, Bela and Beel, J{\"o}ran},
  year = {2009},
  abstract = {This paper presents an approach for identifying similar documents that can be used to assist scientists in finding related work. The approach called Citation Proximity Analysis (CPA) is a further development of co-citation analysis, but in addition, considers the proximity of citations to each other within an article's full-text. The underlying idea is that the closer citations are to each other, the more likely it is that they are related. In comparison to existing approaches, such as bibliographic coupling, co-citation analysis or keyword based approaches the advantages of CPA are a higher precision and the possibility to identify related sections within documents. Moreover, CPA allows a more precise automatic document classification. CPA is used as the primary approach to analyse the similarity and to classify the 1.2 million publications contained in the research paper recommender system Scienstein.org.},
  langid = {english}
}

@inproceedings{GippSciensteinResearch2009,
  title = {Scienstein : {{A Research Paper Recommender System}}},
  shorttitle = {Scienstein},
  author = {Gipp, Bela and Beel, J. and Hentschel, C.},
  year = {2009},
  url = {https://www.semanticscholar.org/paper/Scienstein-%3A-A-Research-Paper-Recommender-System-Gipp-Beel/deab9886bd1f39bea5b85fa76ca8f705fec9a85c},
  urldate = {2023-03-01},
  abstract = {This paper introduces Scienstein, the first hybrid research paper recommender system and a powerful alternative to currently used academic search engines. Scienstein improves the approach of the usually used keyword-based search by combining it with citation analysis, author analysis, source analysis, implicit ratings, explicit ratings and in addition, innovative and yet unused methods like the `Distance Similarity Index' (DSI) and the `In-text Impact Factor' (ItIF). Instead of entering just keywords, a user may provide entire documents, including reference lists as input and make implicit and explicit ratings to improve recommendations. With citation, author and source analysis, similar and related documents are easily determinable. All these techniques are managed by a user-friendly GUI.}
}

@article{GoldbergUsingCollaborative1992,
  title = {Using Collaborative Filtering to Weave an Information Tapestry},
  author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
  year = {1992},
  month = dec,
  journal = {Communications of the ACM},
  volume = {35},
  number = {12},
  pages = {61--70},
  issn = {0001-0782},
  doi = {10.1145/138859.138867},
  url = {https://dl.acm.org/doi/10.1145/138859.138867},
  urldate = {2023-08-16}
}

@misc{GoogleDevelopersRecommenderSystem2022,
  title = {Recommender {{System Online Course}}},
  author = {Google Developers},
  year = {2022},
  url = {https://developers.google.com/machine-learning/recommendation},
  urldate = {2023-03-03},
  langid = {english}
}

@misc{GraveLearningWord2018,
  title = {Learning {{Word Vectors}} for 157 {{Languages}}},
  author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  year = {2018},
  month = mar,
  number = {arXiv:1802.06893},
  eprint = {1802.06893},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06893},
  url = {http://arxiv.org/abs/1802.06893},
  urldate = {2023-08-03},
  abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
  archiveprefix = {arxiv}
}

@article{HabibSectionsbasedBibliographic2019,
  title = {Sections-Based Bibliographic Coupling for Research Paper Recommendation},
  author = {Habib, Raja and Afzal, Muhammad Tanvir},
  year = {2019},
  month = may,
  journal = {Scientometrics},
  volume = {119},
  number = {2},
  pages = {643--656},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03053-8},
  url = {http://link.springer.com/10.1007/s11192-019-03053-8},
  urldate = {2023-03-23},
  abstract = {Digital libraries suffer from the problem of information overload due to immense proliferation of research papers in journals and conference papers. This makes it challenging for researchers to access the relevant research papers. Fortunately, research paper recommendation systems offer a solution to this dilemma by filtering all the available information and delivering what is most relevant to the user. Researchers have proposed numerous approaches for research paper recommendation which are based on metadata, content, citation analysis, collaborative filtering, etc. Approaches based on citation analysis, including co-citation and bibliographic coupling, have proven to be significant. Researchers have extended the co-citation approach to include content analysis and citation proximity analysis and this has led to improvement in the accuracy of recommendations. However, in co-citation analysis, similarity between papers is discovered based on the frequency of co-cited papers in different research papers that can belong to different areas. Bibliographic coupling, on the other hand, determines the relevance between two papers based on their common references. Therefore, bibliographic coupling has inherited the benefits of recommending relevant papers; however, traditional bibliographic coupling does not consider the citing patterns of common references in different logical sections of the citing papers. Since the use of citation proximity analysis in co-citation has improved the accuracy of paper recommendation, this paper proposes a paper recommendation approach that extends the traditional bibliographic coupling by exploiting the distribution of citations in logical sections in bibliographically coupled papers. Comprehensive automated evaluation utilizing Jensen Shannon Divergence was conducted to evaluate the proposed approach. The results showed significant improvement over traditional bibliographic coupling and content-based research paper recommendation.},
  langid = {english}
}

@article{HarrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {\emph{WORD}},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
  urldate = {2023-08-21},
  langid = {english}
}

@article{HeDeepResidual2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2022-01-22},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv}
}

@misc{HeNeuralCollaborative2017,
  title = {Neural {{Collaborative Filtering}}},
  author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  year = {2017},
  month = aug,
  number = {arXiv:1708.05031},
  eprint = {1708.05031},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1708.05031},
  url = {http://arxiv.org/abs/1708.05031},
  urldate = {2023-06-30},
  abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
  archiveprefix = {arxiv}
}

@article{HerlockerEvaluatingCollaborative2004,
  title = {Evaluating Collaborative Filtering Recommender Systems},
  author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
  year = {2004},
  month = jan,
  journal = {ACM Transactions on Information Systems},
  volume = {22},
  number = {1},
  pages = {5--53},
  issn = {1046-8188},
  doi = {10.1145/963770.963772},
  url = {https://dl.acm.org/doi/10.1145/963770.963772},
  urldate = {2023-04-12},
  abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.}
}

@inproceedings{HonnibalImprovedNonmonotonic2015,
  title = {An Improved Non-Monotonic Transition System for Dependency Parsing},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Honnibal, Matthew and Johnson, Mark},
  year = {2015},
  month = sep,
  pages = {1373--1378},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  url = {https://aclweb.org/anthology/D/D15/D15-1162}
}

@misc{HouLargeLanguage2023,
  title = {Large {{Language Models}} Are {{Zero-Shot Rankers}} for {{Recommender Systems}}},
  author = {Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin},
  year = {2023},
  month = may,
  number = {arXiv:2305.08845},
  eprint = {2305.08845},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.08845},
  url = {http://arxiv.org/abs/2305.08845},
  urldate = {2023-07-21},
  abstract = {Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.},
  archiveprefix = {arxiv}
}

@inproceedings{HuCollaborativeFiltering2008,
  title = {Collaborative {{Filtering}} for {{Implicit Feedback Datasets}}},
  booktitle = {Proceedings - {{IEEE International Conference}} on {{Data Mining}}, {{ICDM}}},
  author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
  year = {2008},
  month = dec,
  pages = {263--272},
  doi = {10.1109/ICDM.2008.22},
  abstract = {A common task of recommender systems is to improve customer experience through personalized recommenda- tions based on prior implicit feedback. These systems pas- sively track different sorts of user behavior, such as pur- chase history, watching habits and browsing activity, in or- der to model user preferences. Unlike the much more ex- tensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique proper- ties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference asso- ciated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feed- back recommenders. We also suggest a scalable optimiza- tion procedure, which scales linearly with the data size. The algorithmis used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.}
}

@article{JaccardEtudeDistribution1901,
  title = {Etude de La Distribution Florale Dans Une Portion Des {{Alpes}} et Du {{Jura}}},
  author = {Jaccard, Paul},
  year = {1901},
  month = jan,
  journal = {Bulletin de la Societe Vaudoise des Sciences Naturelles},
  volume = {37},
  pages = {547--579},
  doi = {10.5169/seals-266450},
  abstract = {Common error in bibliographies: "\'Etude comparative de la distribution florale dans une portion des Alpes et des Jura".}
}

@article{JiSurveyHallucination2023,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  year = {2023},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {12},
  eprint = {2202.03629},
  primaryclass = {cs},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3571730},
  url = {http://arxiv.org/abs/2202.03629},
  urldate = {2023-09-17},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG. CCS Concepts: \textbullet{} Computing methodologies \textrightarrow{} Natural language generation; Neural networks.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{JoshiBERTCoreference2019,
  title = {{{BERT}} for {{Coreference Resolution}}: {{Baselines}} and {{Analysis}}},
  shorttitle = {{{BERT}} for {{Coreference Resolution}}},
  author = {Joshi, Mandar and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
  year = {2019},
  month = dec,
  number = {arXiv:1908.09091},
  eprint = {1908.09091},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.09091},
  url = {http://arxiv.org/abs/1908.09091},
  urldate = {2023-08-06},
  abstract = {We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available.},
  archiveprefix = {arxiv}
}

@misc{JurafskySpeechLanguage2022,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James},
  year = {2022},
  month = jan
}

@article{KaminskasDiversitySerendipity2016,
  title = {Diversity, {{Serendipity}}, {{Novelty}}, and {{Coverage}}: {{A Survey}} and {{Empirical Analysis}} of {{Beyond-Accuracy Objectives}} in {{Recommender Systems}}},
  shorttitle = {Diversity, {{Serendipity}}, {{Novelty}}, and {{Coverage}}},
  author = {Kaminskas, Marius and Bridge, Derek},
  year = {2016},
  month = dec,
  journal = {ACM Transactions on Interactive Intelligent Systems},
  volume = {7},
  number = {1},
  pages = {2:1--2:42},
  issn = {2160-6455},
  doi = {10.1145/2926720},
  url = {https://doi.org/10.1145/2926720},
  urldate = {2023-03-20},
  abstract = {What makes a good recommendation or good list of recommendations? Research into recommender systems has traditionally focused on accuracy, in particular how closely the recommender's predicted ratings are to the users' true ratings. However, it has been recognized that other recommendation qualities\textemdash such as whether the list of recommendations is diverse and whether it contains novel items\textemdash may have a significant impact on the overall quality of a recommender system. Consequently, in recent years, the focus of recommender systems research has shifted to include a wider range of ``beyond accuracy'' objectives. In this article, we present a survey of the most discussed beyond-accuracy objectives in recommender systems research: diversity, serendipity, novelty, and coverage. We review the definitions of these objectives and corresponding metrics found in the literature. We also review works that propose optimization strategies for these beyond-accuracy objectives. Since the majority of works focus on one specific objective, we find that it is not clear how the different objectives relate to each other. Hence, we conduct a set of offline experiments aimed at comparing the performance of different optimization approaches with a view to seeing how they affect objectives other than the ones they are optimizing. We use a set of state-of-the-art recommendation algorithms optimized for recall along with a number of reranking strategies for optimizing the diversity, novelty, and serendipity of the generated recommendations. For each reranking strategy, we measure the effects on the other beyond-accuracy objectives and demonstrate important insights into the correlations between the discussed objectives. For instance, we find that rating-based diversity is positively correlated with novelty, and we demonstrate the positive influence of novelty on recommendation coverage.}
}

@inproceedings{KanakiaScalableHybrid2019,
  title = {A {{Scalable Hybrid Research Paper Recommender System}} for {{Microsoft Academic}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Kanakia, Anshul and Shen, Zhihong and Eide, Darrin and Wang, Kuansan},
  year = {2019},
  month = may,
  eprint = {1905.08880},
  primaryclass = {cs},
  pages = {2893--2899},
  doi = {10.1145/3308558.3313700},
  url = {http://arxiv.org/abs/1905.08880},
  urldate = {2023-03-23},
  abstract = {We present the design and methodology for the large scale hybrid paper recommender system used by Microsoft Academic. The system provides recommendations for approximately 160 million English research papers and patents. Our approach handles incomplete citation information while also alleviating the cold-start problem that often affects other recommender systems. We use the Microsoft Academic Graph (MAG), titles, and available abstracts of research papers to build a recommendation list for all documents, thereby combining co-citation and content based approaches. Tuning system parameters also allows for blending and prioritization of each approach which, in turn, allows us to balance paper novelty versus authority in recommendation results. We evaluate the generated recommendations via a user study of 40 participants, with over 2400 recommendation pairs graded and discuss the quality of the results using P@10 and nDCG scores. We see that there is a strong correlation between participant scores and the similarity rankings produced by our system but that additional focus needs to be put towards improving recommender precision, particularly for content based recommendations. The results of the user survey and associated analysis scripts are made available via GitHub and the recommendations produced by our system are available as part of the MAG on Azure to facilitate further research and light up novel research paper recommendation applications.},
  archiveprefix = {arxiv}
}

@article{KandimallaLargeScale2021,
  title = {Large {{Scale Subject Category Classification}} of {{Scholarly Papers With Deep Attentive Neural Networks}}},
  author = {Kandimalla, Bharath and Rohatgi, Shaurya and Wu, Jian and Giles, C. Lee},
  year = {2021},
  journal = {Frontiers in Research Metrics and Analytics},
  volume = {5},
  issn = {2504-0537},
  url = {https://www.frontiersin.org/articles/10.3389/frma.2020.600382},
  urldate = {2023-03-15},
  abstract = {Subject categories of scholarly papers generally refer to the knowledge domain(s) to which the papers belong, examples being computer science or physics. Subject category classification is a prerequisite for bibliometric studies, organizing scientific publications for domain knowledge extraction, and facilitating faceted searches for digital library search engines. Unfortunately, many academic papers do not have such information as part of their metadata. Most existing methods for solving this task focus on unsupervised learning that often relies on citation networks. However, a complete list of papers citing the current paper may not be readily available. In particular, new papers that have few or no citations cannot be classified using such methods. Here, we propose a deep attentive neural network (DANN) that classifies scholarly papers using only their abstracts. The network is trained using nine million abstracts from Web of Science (WoS). We also use the WoS schema that covers 104 subject categories. The proposed network consists of two bi-directional recurrent neural networks followed by an attention layer. We compare our model against baselines by varying the architecture and text representation. Our best model achieves micro-F1 measure of 0.76 with F1 of individual subject categories ranging from 0.50 to 0.95. The results showed the importance of retraining word embedding models to maximize the vocabulary overlap and the effectiveness of the attention mechanism. The combination of word vectors with TFIDF outperforms character and sentence level embedding models. We discuss imbalanced samples and overlapping categories and suggest possible strategies for mitigation. We also determine the subject category distribution in CiteSeerX by classifying a random sample of one million academic papers.}
}

@article{KesslerBibliographicCoupling1963,
  title = {Bibliographic Coupling between Scientific Papers},
  author = {Kessler, M. M.},
  year = {1963},
  month = jan,
  journal = {American Documentation},
  volume = {14},
  number = {1},
  pages = {10--25},
  issn = {0096946X, 19366108},
  doi = {10.1002/asi.5090140103},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.5090140103},
  urldate = {2023-03-25},
  abstract = {This report describes the results of automatic processing of a large number of scientific papers according to a rigorously defined criterion of coupling. The population of papers under study was ordered into groups that satisfy the stated criterion of interrelation. An examination of the papers that constitute the groups shows a high degree of logical correlation.},
  langid = {english}
}

@article{KhabsaNumberScholarly2014,
  title = {The {{Number}} of {{Scholarly Documents}} on the {{Public Web}}},
  author = {Khabsa, Madian and Giles, C. Lee},
  year = {2014},
  month = may,
  journal = {PLOS ONE},
  volume = {9},
  number = {5},
  pages = {e93949},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0093949},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0093949},
  urldate = {2023-08-20},
  abstract = {The number of scholarly documents available on the web is estimated using capture/recapture methods by studying the coverage of two major academic search engines: Google Scholar and Microsoft Academic Search. Our estimates show that at least 114 million English-language scholarly documents are accessible on the web, of which Google Scholar has nearly 100 million. Of these, we estimate that at least 27 million (24\%) are freely available since they do not require a subscription or payment of any kind. In addition, at a finer scale, we also estimate the number of scholarly documents on the web for fifteen fields: Agricultural Science, Arts and Humanities, Biology, Chemistry, Computer Science, Economics and Business, Engineering, Environmental Sciences, Geosciences, Material Science, Mathematics, Medicine, Physics, Social Sciences, and Multidisciplinary, as defined by Microsoft Academic Search. In addition, we show that among these fields the percentage of documents defined as freely available varies significantly, i.e., from 12 to 50\%.},
  langid = {english}
}

@inproceedings{KhadkaCapturingExploiting2020,
  title = {Capturing and {{Exploiting Citation Knowledge}} for {{Recommending Recently Published Papers}}},
  booktitle = {2020 {{IEEE}} 29th {{International Conference}} on {{Enabling Technologies}}: {{Infrastructure}} for {{Collaborative Enterprises}} ({{WETICE}})},
  author = {Khadka, Anita and Cantador, Iv{\'a}n and Fernandez, Miriam},
  year = {2020},
  month = sep,
  pages = {239--244},
  issn = {2641-8169},
  doi = {10.1109/WETICE49692.2020.00054},
  abstract = {With the continuous growth of scientific literature, discovering relevant academic papers for a researcher has become a challenging task, especially when looking for the latest, most recent papers. In this case, traditional collaborative filtering systems are ineffective, since they are unable to recommend items not previously seen, rated or cited. This is known as the item cold-start problem. In this paper, we explore the potential of exploiting citation knowledge to provide a given user with relevant suggestions about recent scientific publications. A novel hybrid recommendation method that encapsulates such citation knowledge is proposed. Experimental results show improvements over baseline methods, evidencing benefits of using citation knowledge to recommend recently published papers in a personalised way. Moreover, as a result of our work, we also provide a unique dataset that, differently to previous corpora, contains detailed paper citation information.}
}

@article{KnothCanWe2017,
  title = {Can We Do Better than {{Co-Citations}}? - {{Bringing Citation Proximity Analysis}} from Idea to Practice in Research Article Recommendation},
  author = {Knoth, Petr and Khadka, Anita},
  year = {2017},
  abstract = {In this paper, we build on the idea of Citation Proximity Analysis (CPA), originally introduced in [1], by developing a step by step scalable approach for building CPA-based recommender systems. As part of this approach, we introduce three new proximity functions, extending the basic assumption of co-citation analysis (stating that the more often two articles are co-cited in a document, the more likely they are related) to take the distance between the co-cited documents into account. Asking the question of whether CPA can outperform co-citation analysis in recommender systems, we have built a CPA based recommender system from a corpus of 368,385 full-texts articles and conducted a user survey to perform an initial evaluation. Two of our three proximity functions used within CPA outperform co-citations on our evaluation dataset.},
  langid = {english}
}

@article{KorenMatrixFactorization2009,
  title = {Matrix {{Factorization Techniques}} for {{Recommender Systems}}},
  author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  year = {2009},
  month = aug,
  journal = {Computer},
  volume = {42},
  number = {8},
  pages = {30--37},
  issn = {0018-9162},
  doi = {10.1109/MC.2009.263},
  url = {http://ieeexplore.ieee.org/document/5197422/},
  urldate = {2023-07-19},
  langid = {english}
}

@misc{LeDistributedRepresentations2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  month = may,
  number = {arXiv:1405.4053},
  eprint = {1405.4053},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1405.4053},
  url = {http://arxiv.org/abs/1405.4053},
  urldate = {2023-03-01},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archiveprefix = {arxiv}
}

@misc{LiGPT4RecGenerative2023,
  title = {{{GPT4Rec}}: {{A Generative Framework}} for {{Personalized Recommendation}} and {{User Interests Interpretation}}},
  shorttitle = {{{GPT4Rec}}},
  author = {Li, Jinming and Zhang, Wentao and Wang, Tian and Xiong, Guanglei and Lu, Alan and Medioni, Gerard},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03879},
  eprint = {2304.03879},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.03879},
  url = {http://arxiv.org/abs/2304.03879},
  urldate = {2023-07-21},
  abstract = {Recent advancements in Natural Language Processing (NLP) have led to the development of NLP-based recommender systems that have shown superior performance. However, current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories. To address these limitations, we present GPT4Rec, a novel and flexible generative framework inspired by search engines. It first generates hypothetical "search queries" given item titles in a user's history, and then retrieves items for recommendation by searching these queries. The framework overcomes previous limitations by learning both user and item embeddings in the language space. To well-capture user interests with different aspects and granularity for improving relevance and diversity, we propose a multi-query generation technique with beam search. The generated queries naturally serve as interpretable representations of user interests and can be searched to recommend cold-start items. With GPT-2 language model and BM25 search engine, our framework outperforms state-of-the-art methods by \$75.7\textbackslash\%\$ and \$22.2\textbackslash\%\$ in Recall@K on two public datasets. Experiments further revealed that multi-query generation with beam search improves both the diversity of retrieved items and the coverage of a user's multi-interests. The adaptiveness and interpretability of generated queries are discussed with qualitative case studies.},
  archiveprefix = {arxiv}
}

@article{LindenAmazonCom2003,
  title = {Amazon.Com Recommendations: Item-to-Item Collaborative Filtering},
  shorttitle = {Amazon.Com Recommendations},
  author = {Linden, G. and Smith, B. and York, J.},
  year = {2003},
  month = jan,
  journal = {IEEE Internet Computing},
  volume = {7},
  number = {1},
  pages = {76--80},
  issn = {1089-7801},
  doi = {10.1109/MIC.2003.1167344},
  url = {http://ieeexplore.ieee.org/document/1167344/},
  urldate = {2023-07-19},
  langid = {english}
}

@misc{LinHowCan2023,
  title = {How {{Can Recommender Systems Benefit}} from {{Large Language Models}}: {{A Survey}}},
  shorttitle = {How {{Can Recommender Systems Benefit}} from {{Large Language Models}}},
  author = {Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu, Weiwen and Chen, Bo and Li, Xiangyang and Zhu, Chenxu and Guo, Huifeng and Yu, Yong and Tang, Ruiming and Zhang, Weinan},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05817},
  eprint = {2306.05817},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.05817},
  url = {http://arxiv.org/abs/2306.05817},
  urldate = {2023-07-21},
  abstract = {Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, and whether to involve conventional recommendation model (CRM) for inference. Detailed analysis and general development trajectories are provided for both questions, respectively. Then, we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. We also actively maintain a GitHub repository for papers and other related resources in this rising direction: https://github.com/CHIANGEL/Awesome-LLM-for-RecSys.},
  archiveprefix = {arxiv}
}

@misc{LiuChatGPTGood2023,
  title = {Is {{ChatGPT}} a {{Good Recommender}}? {{A Preliminary Study}}},
  shorttitle = {Is {{ChatGPT}} a {{Good Recommender}}?},
  author = {Liu, Junling and Liu, Chao and Zhou, Peilin and Lv, Renjie and Zhou, Kang and Zhang, Yan},
  year = {2023},
  month = jun,
  number = {arXiv:2304.10149},
  eprint = {2304.10149},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2304.10149},
  urldate = {2023-07-21},
  abstract = {Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.},
  archiveprefix = {arxiv}
}

@inproceedings{LiuEffectsCocitation2011,
  title = {The {{Effects}} of {{Co-citation Proximity}} on {{Co-citation Analysis}}},
  author = {Liu, Shengbo and Chen, Chaomei},
  year = {2011},
  url = {https://www.semanticscholar.org/paper/The-Effects-of-Co-citation-Proximity-on-Co-citation-Liu-Chen/ac860530cc300768b963a90a46cbffb9b20ef540},
  urldate = {2023-08-16},
  abstract = {In this paper we investigate the effects of co-citation proximity on the quality of co-citation analysis through four experiments of co-citation instances found in full-text scientific publications. First, we compared the distributions of co-citation instances at four levels of proximity in journal articles with the traditionally used article-level co-citation counts. Second, we analyzed how co-citation instances at different proximity levels are distributed across organizational sections in articles. Third, the distribution of co-citation proximity over different co-citation frequency groups is investigated. Fourth, we identified the occurrences of co-citations at different proximity levels with reference to the corresponding traditional co-citation network. The results show that sentence-level co-citations not only preserve the essential structure of the corresponding traditional cocitation network but also form a much smaller subset of the entire co-citation instances typically considered by traditional co-citation analysis. Implications for improving our understanding of underlying factors concerning co-citations and developing more efficient co-citation analysis methods are discussed.}
}

@misc{LiuPretrainPrompt2023,
  title = {Pre-Train, {{Prompt}} and {{Recommendation}}: {{A Comprehensive Survey}} of {{Language Modelling Paradigm Adaptations}} in {{Recommender Systems}}},
  shorttitle = {Pre-Train, {{Prompt}} and {{Recommendation}}},
  author = {Liu, Peng and Zhang, Lemei and Gulla, Jon Atle},
  year = {2023},
  month = mar,
  number = {arXiv:2302.03735},
  eprint = {2302.03735},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.03735},
  urldate = {2023-07-21},
  abstract = {The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and different input data types for recommender systems. Finally, we elaborate on open issues and future research directions in this vibrant field.},
  archiveprefix = {arxiv}
}

@misc{LiuRoBERTaRobustly2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.11692},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2023-08-06},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv}
}

@inproceedings{LopezGROBIDCombining2009,
  title = {{{GROBID}}: {{Combining Automatic Bibliographic Data Recognition}} and {{Term Extraction}} for {{Scholarship Publications}}},
  shorttitle = {{{GROBID}}},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Lopez, Patrice},
  editor = {Agosti, Maristella and Borbinha, Jos{\'e} and Kapidakis, Sarantos and Papatheodorou, Christos and Tsakonas, Giannis},
  year = {2009},
  volume = {5714},
  pages = {473--474},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-04346-8_62},
  url = {http://link.springer.com/10.1007/978-3-642-04346-8_62},
  urldate = {2023-03-07},
  abstract = {Based on state of the art machine learning techniques, GROBID (GeneRation Of BIbliographic Data) performs reliable bibliographic data extractions from scholar articles combined with multi-level term extractions. These two types of extraction present synergies and correspond to complementary descriptions of an article. This tool is viewed as a component for enhancing the existing and the future large repositories of technical and scientific publications.},
  isbn = {978-3-642-04345-1 978-3-642-04346-8}
}

@misc{LuongEffectiveApproaches2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  month = sep,
  number = {arXiv:1508.04025},
  eprint = {1508.04025},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1508.04025},
  url = {http://arxiv.org/abs/1508.04025},
  urldate = {2023-08-06},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  archiveprefix = {arxiv}
}

@article{LvLowerboundingTerm2011,
  title = {Lower-Bounding Term Frequency Normalization: 20th {{ACM Conference}} on {{Information}} and {{Knowledge Management}}, {{CIKM}}'11},
  shorttitle = {Lower-Bounding Term Frequency Normalization},
  author = {Lv, Yuanhua and Zhai, Chengxiang},
  year = {2011},
  journal = {CIKM'11 - Proceedings of the 2011 ACM International Conference on Information and Knowledge Management},
  series = {International {{Conference}} on {{Information}} and {{Knowledge Management}}, {{Proceedings}}},
  pages = {7--16},
  issn = {9781450307178},
  doi = {10.1145/2063576.2063584},
  url = {http://www.scopus.com/inward/record.url?scp=83055187814&partnerID=8YFLogxK},
  urldate = {2023-05-11},
  abstract = {In this paper, we reveal a common deficiency of the current retrieval models: the component of term frequency (TF) normalization by document length is not lower-bounded properly; as a result, very long documents tend to be overly penalized. In order to analytically diagnose this problem, we propose two desirable formal constraints to capture the heuristic of lower-bounding TF, and use constraint analysis to examine several representative retrieval functions. Analysis results show that all these retrieval functions can only satisfy the constraints for a certain range of parameter values and/or for a particular set of query terms. Empirical results further show that the retrieval performance tends to be poor when the parameter is out of the range or the query term is not in the particular set. To solve this common problem, we propose a general and efficient method to introduce a sufficiently large lower bound for TF normalization which can be shown analytically to fix or alleviate the problem. Our experimental results demonstrate that the proposed method, incurring almost no additional computational cost, can be applied to state-of-the-art retrieval functions, such as Okapi BM25, language models, and the divergence from randomness approach, to significantly improve the average precision, especially for verbose queries.}
}

@book{ManningIntroductionInformation2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  month = jul,
  publisher = {{Cambridge University Press}},
  abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
  googlebooks = {GNvtngEACAAJ},
  isbn = {978-0-521-86571-5},
  langid = {english}
}

@inproceedings{Marshakova-ShaikevichSystemDocument1973,
  title = {System of {{Document Connections Based}} on {{References}}},
  author = {{Marshakova-Shaikevich}, Irena},
  year = {1973},
  url = {https://www.semanticscholar.org/paper/System-of-Document-Connections-Based-on-References-Marshakova-shaikevich/2d871489eb7288dd1bec4be99bc363efd4933d48},
  urldate = {2023-03-07},
  abstract = {Analysis of bibliographic references is becoming a tool of studying information processes in science, of classification of documents. In order to classifL documents in a field of knowledge it is important to study all references in the documents, connected with the given direction of research. The numerically expressed data of citing practices within the field should be followed by hrther conceptual analysis.}
}

@inproceedings{MassAdhocDocument2020,
  title = {Ad-Hoc {{Document Retrieval}} Using {{Weak-Supervision}} with {{BERT}} and {{GPT2}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Mass, Yosi and Roitman, Haggai},
  year = {2020},
  month = nov,
  pages = {4191--4197},
  doi = {10.18653/v1/2020.emnlp-main.343},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.343},
  urldate = {2023-04-25},
  abstract = {Yosi Mass, Haggai Roitman. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.},
  langid = {american}
}

@misc{McCormickBERTWord2019,
  title = {{{BERT Word Embeddings Tutorial}}},
  author = {McCormick, Chris},
  year = {2019},
  month = may,
  url = {https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#sentence-vectors},
  urldate = {2023-03-08}
}

@misc{McCormickWord2VecTutorial2016,
  title = {{{Word2Vec Tutorial}} - {{The Skip-Gram Model}}},
  author = {McCormick, Chris},
  year = {2016},
  url = {http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/},
  urldate = {2023-03-01}
}

@misc{MikolovEfficientEstimation2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2023-03-01},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arxiv}
}

@misc{MundlerSelfcontradictoryHallucinations2023,
  title = {Self-Contradictory {{Hallucinations}} of {{Large Language Models}}: {{Evaluation}}, {{Detection}} and {{Mitigation}}},
  shorttitle = {Self-Contradictory {{Hallucinations}} of {{Large Language Models}}},
  author = {M{\"u}ndler, Niels and He, Jingxuan and Jenko, Slobodan and Vechev, Martin},
  year = {2023},
  month = may,
  number = {arXiv:2305.15852},
  eprint = {2305.15852},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.15852},
  urldate = {2023-09-17},
  abstract = {Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify selfcontradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0\% precision and 80.5\% recall on the sentence pairs generated by itself. To automatically mitigate self-contradictions, we develop an iterative algorithm that prompts the LMs to remove the detected self-contradictions from the generated text. Our algorithm successfully revises the text such that self-contradictions are significantly reduced, while maintaining its fluency and informativeness. Importantly, our entire pipeline of triggering, detecting, and mitigating self-contradictions is applicable to black-box LMs and does not require any external grounded knowledge.},
  archiveprefix = {arxiv},
  langid = {english}
}

@inproceedings{NascimentoSourceIndependent2011,
  title = {A Source Independent Framework for Research Paper Recommendation},
  booktitle = {Proceedings of the 11th Annual International {{ACM}}/{{IEEE}} Joint Conference on {{Digital}} Libraries},
  author = {Nascimento, Cristiano and Laender, Alberto H.F. and {da Silva}, Altigran S. and Gon{\c c}alves, Marcos Andr{\'e}},
  year = {2011},
  month = jun,
  pages = {297--306},
  publisher = {{ACM}},
  address = {{Ottawa Ontario Canada}},
  doi = {10.1145/1998076.1998132},
  url = {https://dl.acm.org/doi/10.1145/1998076.1998132},
  urldate = {2023-03-30},
  abstract = {As the number of research papers available on the Web has increased enormously over the years, paper recommender systems have been proposed to help researchers on automatically finding works of interest. The main problem with the current approaches is that they assume that recommending algorithms are provided with a rich set of evidence (e.g., document collections, citations, profiles) which is normally not widely available. In this paper we propose a novel source independent framework for research paper recommendation. The framework requires as input only a single research paper and generates several potential queries by using terms in that paper, which are then submitted to existing Web information sources that hold research papers. Once a set of candidate papers for recommendation is generated, the framework applies content-based recommending algorithms to rank the candidates in order to recommend the ones most related to the input paper. This is done by using only publicly available metadata (i.e., title and abstract). We evaluate our proposed framework by performing an extensive experimentation in which we analyzed several strategies for query generation and several ranking strategies for paper recommendation. Our results show that good recommendations can be obtained with simple and low cost strategies.},
  isbn = {978-1-4503-0744-4},
  langid = {english}
}

@misc{NishimuraBestDocument2021,
  title = {The {{Best Document Similarity Algorithm}} in 2020 {{A Beginner}}'s {{Guide}}},
  shorttitle = {The {{Best Document Similarity Algorithm}} in 2020},
  author = {Nishimura, Masatoshi},
  year = {2021},
  month = may,
  journal = {Medium},
  url = {https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05},
  urldate = {2023-02-27},
  abstract = {If you want to know the best algorithm on document similarity task in 2020, you've come to the right place.},
  langid = {english}
}

@misc{OpenAIGPT4Technical2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-07-28},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv}
}

@misc{OstendorffAspectbasedDocument2020,
  title = {Aspect-Based {{Document Similarity}} for {{Research Papers}}},
  author = {Ostendorff, Malte and Ruas, Terry and Blume, Till and Gipp, Bela and Rehm, Georg},
  year = {2020},
  month = oct,
  number = {arXiv:2010.06395},
  eprint = {2010.06395},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.06395},
  url = {http://arxiv.org/abs/2010.06395},
  urldate = {2023-03-23},
  abstract = {Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity for research papers. Paper citations indicate the aspect-based similarity, i.e., the section title in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our results show SciBERT as the best performing system. A qualitative examination validates our quantitative results. Our findings motivate future research of aspect-based document similarity and the development of a recommender system based on the evaluated techniques. We make our datasets, code, and trained models publicly available.},
  archiveprefix = {arxiv}
}

@misc{OstendorffPairwiseMultiClass2020,
  title = {Pairwise {{Multi-Class Document Classification}} for {{Semantic Relations}} between {{Wikipedia Articles}}},
  author = {Ostendorff, Malte and Ruas, Terry and Schubotz, Moritz and Rehm, Georg and Gipp, Bela},
  year = {2020},
  month = mar,
  number = {arXiv:2003.09881},
  eprint = {2003.09881},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.09881},
  url = {http://arxiv.org/abs/2003.09881},
  urldate = {2023-03-10},
  abstract = {Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph-Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of recommender systems based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.},
  archiveprefix = {arxiv}
}

@misc{OstendorffSpecializedDocument2022,
  title = {Specialized {{Document Embeddings}} for {{Aspect-based Similarity}} of {{Research Papers}}},
  author = {Ostendorff, Malte and Blume, Till and Ruas, Terry and Gipp, Bela and Rehm, Georg},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14541},
  eprint = {2203.14541},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.14541},
  url = {http://arxiv.org/abs/2203.14541},
  urldate = {2023-03-10},
  abstract = {Document embeddings and similarity measures underpin content-based recommender systems, whereby a document is commonly represented as a single generic embedding. However, similarity computed on single vector representations provides only one perspective on document similarity that ignores which aspects make two documents alike. To address this limitation, aspect-based similarity measures have been developed using document segmentation or pairwise multi-class document classification. While segmentation harms the document coherence, the pairwise classification approach scales poorly to large scale corpora. In this paper, we treat aspect-based similarity as a classical vector similarity problem in aspect-specific embedding spaces. We represent a document not as a single generic embedding but as multiple specialized embeddings. Our approach avoids document segmentation and scales linearly w.r.t.the corpus size. In an empirical study, we use the Papers with Code corpus containing 157,606 research papers and consider the task, method, and dataset of the respective research papers as their aspects. We compare and analyze three generic document embeddings, six specialized document embeddings and a pairwise classification baseline in the context of research paper recommendations. As generic document embeddings, we consider FastText, SciBERT, and SPECTER. To compute the specialized document embeddings, we compare three alternative methods inspired by retrofitting, fine-tuning, and Siamese networks. In our experiments, Siamese SciBERT achieved the highest scores. Additional analyses indicate an implicit bias of the generic document embeddings towards the dataset aspect and against the method aspect of each research paper. Our approach of aspect-based document embeddings mitigates potential risks arising from implicit biases by making them explicit.},
  archiveprefix = {arxiv}
}

@misc{PalachyDocumentEmbedding2022,
  title = {Document {{Embedding Techniques}}},
  author = {Palachy, Shay},
  year = {2022},
  month = jun,
  journal = {Medium},
  url = {https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d},
  urldate = {2023-03-08},
  abstract = {A review of notable literature on the topic},
  langid = {english}
}

@inproceedings{PazzaniContentBasedRecommendation2007,
  title = {Content-{{Based Recommendation Systems}}},
  booktitle = {The {{Adaptive Web}}},
  author = {Pazzani, Michael J. and Billsus, Daniel},
  editor = {Brusilovsky, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
  year = {2007},
  volume = {4321},
  pages = {325--341},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-72079-9_10},
  url = {http://link.springer.com/10.1007/978-3-540-72079-9_10},
  urldate = {2023-07-19},
  abstract = {This chapter discusses content-based recommendation systems, i.e., systems that recommend an item to a user based upon a description of the item and a profile of the user's interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale. Although the details of various systems differ, content-based recommendation systems share in common a means for describing the items that may be recommended, a means for creating a profile of the user that describes the types of items the user likes, and a means of comparing items to the user profile to determine what to recommend. The profile is often created and updated automatically in response to feedback on the desirability of items that have been presented to the user.},
  isbn = {978-3-540-72078-2},
  langid = {english}
}

@article{PedregosaScikitlearnMachine2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@inproceedings{PenningtonGloveGlobal2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  urldate = {2023-05-01},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english}
}

@misc{PetersDeepContextualized2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05365},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2023-09-15},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arxiv}
}

@article{RadfordLanguageModels2018,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2018},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english}
}

@article{RehurekGensimPython2011,
  title = {Gensim\textendash{{Python}} Framework for Vector Space Modelling},
  author = {Rehurek, Radim and Sojka, Petr},
  year = {2011},
  journal = {NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic},
  volume = {3},
  number = {2}
}

@book{ReidGettingPublished2010,
  title = {Getting {{Published}} in {{International Journals Writing Strategies}} for {{European Social Scientists}}},
  shorttitle = {Getting {{Published}} in {{International Journals}}},
  author = {Reid, Natalie},
  year = {2010},
  publisher = {{NOVA - Norwegian Social Research}},
  googlebooks = {pIDTZwEACAAJ},
  isbn = {978-82-7894-338-0},
  langid = {english}
}

@misc{ReimersSentenceBERTSentence2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.10084},
  url = {http://arxiv.org/abs/1908.10084},
  urldate = {2023-03-08},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv}
}

@inproceedings{ResnickGroupLensOpen1994,
  title = {{{GroupLens}}: An Open Architecture for Collaborative Filtering of Netnews},
  shorttitle = {{{GroupLens}}},
  booktitle = {Proceedings of the 1994 {{ACM}} Conference on {{Computer}} Supported Cooperative Work},
  author = {Resnick, Paul and Iacovou, Neophytos and Suchak, Mitesh and Bergstrom, Peter and Riedl, John},
  year = {1994},
  month = oct,
  series = {{{CSCW}} '94},
  pages = {175--186},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/192844.192905},
  url = {https://dl.acm.org/doi/10.1145/192844.192905},
  urldate = {2023-08-16},
  abstract = {Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.},
  isbn = {978-0-89791-689-9}
}

@incollection{RicciRecommenderSystems2015,
  title = {Recommender {{Systems}}: {{Introduction}} and {{Challenges}}},
  shorttitle = {Recommender {{Systems}}},
  booktitle = {Recommender {{Systems Handbook}}},
  author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
  year = {2015},
  pages = {1--34},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4899-7637-6_1},
  url = {https://doi.org/10.1007/978-1-4899-7637-6_1},
  urldate = {2023-07-19},
  abstract = {Recommender Systems (RSs) are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user. In this introductory chapter, we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters included in this handbook. Additionally, we aim to help the reader navigate the rich and detailed content that this handbook offers.},
  isbn = {978-1-4899-7637-6},
  langid = {english}
}

@misc{RinkMeanAverage2023,
  title = {Mean {{Average Precision}} at {{K}} ({{MAP}}@{{K}}) Clearly Explained},
  author = {Rink, Konstantin},
  year = {2023},
  month = jan,
  journal = {Medium},
  url = {https://towardsdatascience.com/mean-average-precision-at-k-map-k-clearly-explained-538d8e032d2},
  urldate = {2023-03-23},
  abstract = {One of the most popular evaluation metrics for recommender or ranking problems step by step explained},
  langid = {english}
}

@inproceedings{RobertsonOkapiTREC31995,
  title = {Okapi at {{TREC-3}}},
  booktitle = {Overview of the {{Third Text REtrieval Conference}} ({{TREC-3}})},
  author = {Robertson, Stephen and Walker, S. and Jones, S. and {Hancock-Beaulieu}, M. M. and Gatford, M.},
  year = {1995},
  month = jan,
  pages = {109--126},
  url = {https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/},
  urldate = {2023-04-19},
  abstract = {The Okapi software used for TREC-3 was similar to that used in previous TRECs, comprising a low level basic search system and a user interface for the manual search experiments, together with data conversion and inversion utilities. There were also various scripts and programs for generating query terms, running batches of trials and performing evaluation. [\ldots ]},
  langid = {american}
}

@article{RongWord2vecParameter2016,
  title = {Word2vec {{Parameter Learning Explained}}},
  author = {Rong, Xin},
  year = {2016},
  month = jun,
  journal = {arXiv:1411.2738 [cs]},
  eprint = {1411.2738},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1411.2738},
  urldate = {2022-03-22},
  abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.},
  archiveprefix = {arxiv}
}

@article{RoySystematicReview2022,
  title = {A Systematic Review and Research Perspective on Recommender Systems},
  author = {Roy, Deepjyoti and Dutta, Mala},
  year = {2022},
  month = may,
  journal = {Journal of Big Data},
  volume = {9},
  number = {1},
  pages = {59},
  issn = {2196-1115},
  doi = {10.1186/s40537-022-00592-5},
  url = {https://doi.org/10.1186/s40537-022-00592-5},
  urldate = {2023-07-21},
  abstract = {Recommender systems are efficient tools for filtering online information, which is widespread owing to the changing habits of computer users, personalization trends, and emerging access to the internet. Even though the recent recommender systems are eminent in giving precise recommendations, they suffer from various limitations and challenges like scalability, cold-start, sparsity, etc. Due to the existence of various techniques, the selection of techniques becomes a complex work while building application-focused recommender systems. In addition, each technique comes with its own set of features, advantages and disadvantages which raises even more questions, which should be addressed. This paper aims to undergo a systematic review on various recent contributions in the domain of recommender systems, focusing on diverse applications like books, movies, products, etc. Initially, the various applications of each recommender system are analysed. Then, the algorithmic analysis on various recommender systems is performed and a taxonomy is framed that accounts for various components required for developing an effective recommender system. In addition, the datasets gathered, simulation platform, and performance metrics focused on each contribution are evaluated and noted. Finally, this review provides a much-needed overview of the current state of research in this field and points out the existing gaps and challenges to help posterity in developing an efficient recommender system.}
}

@misc{RuasCSInsightsSystem2023,
  title = {{{CS-Insights}}: {{A System}} for {{Analyzing Computer Science Research}}},
  shorttitle = {{{CS-Insights}}},
  author = {Ruas, Terry and Wahle, Jan Philip and K{\"u}ll, Lennart and Mohammad, Saif M. and Gipp, Bela},
  year = {2023},
  month = jan,
  number = {arXiv:2210.06878},
  eprint = {2210.06878},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06878},
  url = {http://arxiv.org/abs/2210.06878},
  urldate = {2023-08-28},
  abstract = {This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.},
  archiveprefix = {arxiv}
}

@article{RuasEnhancedWord2020,
  title = {Enhanced Word Embeddings Using Multi-Semantic Representation through Lexical Chains},
  author = {Ruas, Terry and Ferreira, Charles Henrique Porto and Grosky, William and {de Fran{\c c}a}, Fabr{\'i}cio Olivetti and Medeiros, D{\'e}bora Maria Rossi},
  year = {2020},
  month = sep,
  journal = {Information Sciences},
  volume = {532},
  eprint = {2101.09023},
  primaryclass = {cs},
  pages = {16--32},
  issn = {00200255},
  doi = {10.1016/j.ins.2020.04.048},
  url = {http://arxiv.org/abs/2101.09023},
  urldate = {2023-03-13},
  abstract = {The relationship between words in a sentence often tells us more about the underlying semantic content of a document than its actual words, individually. In this work, we propose two novel algorithms, called Flexible Lexical Chain II and Fixed Lexical Chain II. These algorithms combine the semantic relations derived from lexical chains, prior knowledge from lexical databases, and the robustness of the distributional hypothesis in word embeddings as building blocks forming a single system. In short, our approach has three main contributions: (i) a set of techniques that fully integrate word embeddings and lexical chains; (ii) a more robust semantic representation that considers the latent relation between words in a document; and (iii) lightweight word embeddings models that can be extended to any natural language task. We intend to assess the knowledge of pre-trained models to evaluate their robustness in the document classification task. The proposed techniques are tested against seven word embeddings algorithms using five different machine learning classifiers over six scenarios in the document classification task. Our results show the integration between lexical chains and word embeddings representations sustain state-of-the-art results, even against more complex systems.},
  archiveprefix = {arxiv}
}

@article{RuasMultisenseEmbeddings2019,
  title = {Multi-Sense Embeddings through a Word Sense Disambiguation Process},
  author = {Ruas, Terry and Grosky, William and Aizawa, Akiko},
  year = {2019},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {136},
  eprint = {2101.08700},
  primaryclass = {cs},
  pages = {288--303},
  issn = {09574174},
  doi = {10.1016/j.eswa.2019.06.026},
  url = {http://arxiv.org/abs/2101.08700},
  urldate = {2023-08-21},
  abstract = {Natural Language Understanding has seen an increasing number of publications in the last few years, especially after robust word embeddings models became prominent, when they proved themselves able to capture and represent semantic relationships from massive amounts of data. Nevertheless, traditional models often fall short in intrinsic issues of linguistics, such as polysemy and homonymy. Any expert system that makes use of natural language in its core, can be affected by a weak semantic representation of text, resulting in inaccurate outcomes based on poor decisions. To mitigate such issues, we propose a novel approach called Most Suitable Sense Annotation (MSSA), that disambiguates and annotates each word by its specific sense, considering the semantic effects of its context. Our approach brings three main contributions to the semantic representation scenario: (i) an unsupervised technique that disambiguates and annotates words by their senses, (ii) a multi-sense embeddings model that can be extended to any traditional word embeddings algorithm, and (iii) a recurrent methodology that allows our models to be re-used and their representations refined. We test our approach on six different benchmarks for the word similarity task, showing that our approach can produce state-of-the-art results and outperforms several more complex state-of-the-art systems.},
  archiveprefix = {arxiv}
}

@article{SaltonTermWeighting1987,
  title = {Term {{Weighting Approaches}} in {{Automatic Text Retrieval}}},
  author = {Salton, Gerard and Buckley, Chris},
  year = {1987},
  month = nov,
  publisher = {{Cornell University}},
  url = {https://ecommons.cornell.edu/handle/1813/6721},
  urldate = {2023-07-19},
  abstract = {The experimental evidence accumulated over the past 20 years indicates that  textindexing systems based on the assignment of appropriately weighted single  terms produce retrieval results that are superior to those obtainable with  other more elaborate text representations. These results depend crucially on  the choice of effective term weighting systems. This paper summarizes the  insights gained in automatic term weighting, and provides baseline single  term indexing models with which other more elaborate content analysis  procedures can be compared.},
  langid = {american},
  annotation = {Accepted: 2007-04-23T17:23:03Z}
}

@inproceedings{SarwarItembasedCollaborative2001,
  title = {Item-Based Collaborative Filtering Recommendation Algorithms},
  booktitle = {Proceedings of the 10th International Conference on {{World Wide Web}}},
  author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
  year = {2001},
  pages = {285--295},
  publisher = {{ACM}},
  address = {{Hong Kong Hong Kong}},
  doi = {10.1145/371920.372071},
  url = {https://dl.acm.org/doi/10.1145/371920.372071},
  urldate = {2023-07-19},
  isbn = {978-1-58113-348-6},
  langid = {english}
}

@misc{SchumanSurveyNeuromorphic2017,
  title = {A {{Survey}} of {{Neuromorphic Computing}} and {{Neural Networks}} in {{Hardware}}},
  author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
  year = {2017},
  month = may,
  number = {arXiv:1705.06963},
  eprint = {1705.06963},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.06963},
  url = {http://arxiv.org/abs/1705.06963},
  urldate = {2023-08-06},
  abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
  archiveprefix = {arxiv}
}

@article{SchwarzerEvaluatingLinkbased2016,
  title = {Evaluating {{Link-based Recommendations}} for {{Wikipedia}}},
  author = {Schwarzer, Malte and Schubotz, Moritz and Meuschke, Norman and Breitinger, Corinna and Markl, Volker and Gipp, Bela},
  year = {2016},
  month = jun,
  journal = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
  pages = {191--200},
  publisher = {{ACM}},
  address = {{Newark New Jersey USA}},
  doi = {10.1145/2910896.2910908},
  url = {https://dl.acm.org/doi/10.1145/2910896.2910908},
  urldate = {2023-03-07},
  abstract = {Literature recommender systems support users in filtering the vast and increasing number of documents in digital libraries and on the Web. For academic literature, research has proven the ability of citation-based document similarity measures, such as Co-Citation (CoCit), or Co-Citation Proximity Analysis (CPA) to improve recommendation quality. In this paper, we report on the first large-scale investigation of the performance of the CPA approach in generating literature recommendations for Wikipedia, which is fundamentally different from the academic literature domain. We analyze links instead of citations to generate article recommendations. We evaluate CPA, CoCit, and the Apache Lucene MoreLikeThis (MLT) function, which represents a traditional text-based similarity measure. We use two datasets of 779,716 and 2.57 million Wikipedia articles, the Big Data processing framework Apache Flink, and a ten-node computing cluster. To enable our large-scale evaluation, we derive two quasi-gold standards from the links in Wikipedia's ``See also'' sections and a comprehensive Wikipedia clickstream dataset. Our results show that the citation-based measures CPA and CoCit have complementary strengths compared to the text-based MLT measure. While MLT performs well in identifying narrowly similar articles that share similar words and structure, the citation-based measures are better able to identify topically related information, such as information on the city of a certain university or other technical universities in the region. The CPA approach, which consistently outperformed CoCit, is better suited for identifying a broader spectrum of related articles, as well as popular articles that typically exhibit a higher quality. Additional benefits of the CPA approach are its lower runtime requirements and its language-independence that allows for a cross-language retrieval of articles. We present a manual analysis of exemplary articles to demonstrate and discuss our findings. The raw data and source code of our study, together with a manual on how to use them, are openly available at: https://github.com/wikimedia/citolytics.},
  isbn = {9781450342292},
  langid = {english}
}

@misc{SeitzUnderstandingTFIDF2020,
  title = {Understanding {{TF-IDF}} and {{BM-25}}},
  author = {Seitz, Rudi},
  year = {2020},
  month = mar,
  journal = {KMW Technology},
  url = {https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/},
  urldate = {2023-04-19},
  abstract = {This post will show you precisely how BM25 builds upon TF-IDF, what its parameters do, and why it is so effective.},
  langid = {american}
}

@misc{SennrichNeuralMachine2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = jun,
  number = {arXiv:1508.07909},
  eprint = {1508.07909},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1508.07909},
  url = {http://arxiv.org/abs/1508.07909},
  urldate = {2023-08-06},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  archiveprefix = {arxiv}
}

@misc{ShperberGentleIntroduction2019,
  title = {A Gentle Introduction to {{Doc2Vec}}},
  author = {Shperber, Gidi},
  year = {2019},
  month = nov,
  journal = {Wisio},
  url = {https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e},
  urldate = {2023-02-27},
  abstract = {TL;DR},
  langid = {english}
}

@article{SilveiraHowGood2019,
  title = {How Good Your Recommender System Is? {{A}} Survey on Evaluations in Recommendation},
  shorttitle = {How Good Your Recommender System Is?},
  author = {Silveira, Thiago and Zhang, Min and Lin, Xiao and Liu, Yiqun and Ma, Shaoping},
  year = {2019},
  month = may,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {10},
  number = {5},
  pages = {813--831},
  issn = {1868-808X},
  doi = {10.1007/s13042-017-0762-9},
  url = {https://doi.org/10.1007/s13042-017-0762-9},
  urldate = {2023-02-27},
  abstract = {Recommender Systems have become a very useful tool for a large variety of domains. Researchers have been attempting to improve their algorithms in order to issue better predictions to the users. However, one of the current challenges in the area refers to how to properly evaluate the predictions generated by a recommender system. In the extent of offline evaluations, some traditional concepts of evaluation have been explored, such as accuracy, Root Mean Square Error and P@N for top-k recommendations. In recent years, more research have proposed some new concepts such as novelty, diversity and serendipity. These concepts have been addressed with the goal to satisfy the users' requirements. Numerous definitions and metrics have been proposed in previous work. On the absence of a specific summarization on evaluations of recommendation combining traditional metrics and recent progresses, this paper surveys and organizes the main research that present definitions about concepts and propose metrics or strategies to evaluate recommendations. In addition, this survey also settles the relationship between the concepts, categorizes them according to their objectives and suggests potential future topics on user satisfaction.},
  langid = {english}
}

@article{SinghalModernInformation2001,
  title = {Modern {{Information Retrieval}}: {{A Brief Overview}}},
  author = {Singhal, Amit},
  year = {2001},
  abstract = {For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.},
  langid = {english}
}

@article{SmallCocitationScientific1973,
  title = {Co-Citation in the Scientific Literature: {{A}} New Measure of the Relationship between Two Documents},
  shorttitle = {Co-Citation in the Scientific Literature},
  author = {Small, Henry},
  year = {1973},
  month = jul,
  journal = {Journal of the American Society for Information Science},
  volume = {24},
  number = {4},
  pages = {265--269},
  issn = {00028231, 10974571},
  doi = {10.1002/asi.4630240406},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.4630240406},
  urldate = {2023-03-07},
  abstract = {A new form of document coupling called co-citation is defined as the frequency with which two documents are cited together. The co-citation frequency of two scientific papers can be determined by comparing lists of citing documents in the Science Citation Index and counting identical entries. Networks of co-cited papers can be generated for specific scientific specialties, and an example is drawn from the literature of particle physics. Co-citation patterns are found to differ significantly from bibliographic coupling patterns, but to agree generally with patterns of direct citation. Clusters of co-cited papers provide a new way to study the specialty structure of science. They may provide a new approach to indexing and to the creation of SDI profiles.},
  langid = {english}
}

@article{SmithCitationAnalysis1981,
  title = {Citation {{Analysis}}},
  author = {Smith, Linda C.},
  year = {1981},
  publisher = {{Graduate School of Library and Information Science. University of Illinois at Urbana-Champaign}},
  issn = {0024-2594},
  url = {https://hdl.handle.net/2142/7190},
  urldate = {2023-03-25},
  copyright = {Copyright 1981 Board of Trustees University of Illinois.},
  langid = {english}
}

@inproceedings{SugiyamaExploitingPotential2013,
  title = {Exploiting Potential Citation Papers in Scholarly Paper Recommendation},
  booktitle = {Proceedings of the 13th {{ACM}}/{{IEEE-CS}} Joint Conference on {{Digital}} Libraries},
  author = {Sugiyama, Kazunari and Kan, Min-Yen},
  year = {2013},
  month = jul,
  series = {{{JCDL}} '13},
  pages = {153--162},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2467696.2467701},
  url = {https://dl.acm.org/doi/10.1145/2467696.2467701},
  urldate = {2023-04-12},
  abstract = {To help generate relevant suggestions for researchers, recommendation systems have started to leverage the latent interests in the publication profiles of the researchers themselves. While using such a publication citation network has been shown to enhance performance, the network is often sparse, making recommendation difficult. To alleviate this sparsity, we identify "potential citation papers" through the use of collaborative filtering. Also, as different logical sections of a paper have different significance, as a secondary contribution, we investigate which sections of papers can be leveraged to represent papers effectively. On a scholarly paper recommendation dataset, we show that recommendation accuracy significantly outperforms state-of-the-art recommendation baselines as measured by nDCG and MRR, when we discover potential citation papers using imputed similarities via collaborative filtering and represent candidate papers using both the full text and assigning more weight to the conclusion sections.},
  isbn = {978-1-4503-2077-1}
}

@misc{SunBERT4RecSequential2019,
  title = {{{BERT4Rec}}: {{Sequential Recommendation}} with {{Bidirectional Encoder Representations}} from {{Transformer}}},
  shorttitle = {{{BERT4Rec}}},
  author = {Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  year = {2019},
  month = aug,
  number = {arXiv:1904.06690},
  eprint = {1904.06690},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1904.06690},
  urldate = {2023-07-21},
  abstract = {Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly "see the target item". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.},
  archiveprefix = {arxiv}
}

@misc{TaifiMRRVs2020,
  title = {{{MRR}} vs {{MAP}} vs {{NDCG Rank-Aware Evaluation Metrics And When To Use Them}}},
  shorttitle = {{{MRR}} vs {{MAP}} vs {{NDCG}}},
  author = {Taifi, Moussa},
  year = {2020},
  month = jun,
  journal = {The Startup},
  url = {https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832},
  urldate = {2023-03-03},
  abstract = {Rank-aware evaluation metrics for recommendation systems, and when to use them.},
  langid = {english}
}

@article{TranEnrichingPubMed2009,
  title = {Enriching {{PubMed Related Article Search}} with {{Sentence Level Co-citations}}},
  author = {Tran, Nam and Alves, Pedro and Ma, Shuangge and Krauthammer, Michael},
  year = {2009},
  journal = {AMIA Annual Symposium Proceedings},
  volume = {2009},
  pages = {650--654},
  issn = {1942-597X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815371/},
  urldate = {2023-08-16},
  abstract = {PubMed related article links identify closely related articles and enhance our ability to navigate the biomedical literature. They are derived by calculating the word similarity between two articles, relating articles with overlapping word content. In this paper, we propose to enrich PubMed with a new type of related article link based on citations within a single sentence (i.e. sentence level co-citations or SLCs). Using different similarity metrics, we demonstrated that articles linked by SLCs are highly related. We also showed that only half of SLCs are found among PubMed related article links. Additionally, we discuss how the citing sentence of an SLC explains the connection between two articles.},
  pmcid = {PMC2815371},
  pmid = {20351935}
}

@misc{TrinhSimpleMethod2019,
  title = {A {{Simple Method}} for {{Commonsense Reasoning}}},
  author = {Trinh, Trieu H. and Le, Quoc V.},
  year = {2019},
  month = sep,
  number = {arXiv:1806.02847},
  eprint = {1806.02847},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.02847},
  url = {http://arxiv.org/abs/1806.02847},
  urldate = {2023-08-07},
  abstract = {Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.},
  archiveprefix = {arxiv}
}

@article{TrotmanEfficientEffective2012,
  title = {Towards an {{Efficient}} and {{Effective Search Engine}}},
  author = {Trotman, Andrew and Jia, Xiang-Fei and Crane, Matt},
  year = {2012},
  month = aug,
  abstract = {Building an efficient and effective search engine requires both science and engineering. In this paper, we discuss the ATIRE search engine developed in our research lab, and both the engineering decisions and research questions that have motivated building ATIRE.},
  langid = {english}
}

@inproceedings{TrotmanImprovementsBM252014,
  title = {Improvements to {{BM25}} and {{Language Models Examined}}},
  booktitle = {Proceedings of the 2014 {{Australasian Document Computing Symposium}}},
  author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
  year = {2014},
  month = nov,
  pages = {58--65},
  publisher = {{ACM}},
  address = {{Melbourne VIC Australia}},
  doi = {10.1145/2682862.2682863},
  url = {https://dl.acm.org/doi/10.1145/2682862.2682863},
  urldate = {2023-04-19},
  abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TFl{$\smwhtcircle$}{$\smwhtcircle$}pID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best overall.},
  isbn = {978-1-4503-3000-8},
  langid = {english}
}

@book{TunstallNaturalLanguage2022,
  title = {Natural {{Language Processing}} with {{Transformers}}},
  shorttitle = {Natural {{Language Processing}} with {{Transformers}}},
  author = {Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},
  year = {2022},
  month = feb,
  publisher = {{O'Reilly Media, Incorporated}},
  abstract = {Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or coder, this practical book shows you how to train and scale these large models using Hugging Face Transformers, a Python-based deep learning library.Transformers have been used to write realistic news stories, improve Google Search queries, and even create chatbots that tell corny jokes. In this guide, authors Lewis Tunstall, Leandro von Werra, and Thomas Wolf, among the creators of Hugging Face Transformers, use a hands-on approach to teach you how transformers work and how to integrate them in your applications. You'll quickly learn a variety of tasks they can help you solve.Build, debug, and optimize transformer models for core NLP tasks, such as text classification, named entity recognition, and question answeringLearn how transformers can be used for cross-lingual transfer learningApply transformers in real-world scenarios where labeled data is scarceMake transformer models efficient for deployment using techniques such as distillation, pruning, and quantizationTrain transformers from scratch and learn how to scale to multiple GPUs and distributed environments},
  googlebooks = {7hhyzgEACAAJ},
  isbn = {978-1-09-810324-8},
  langid = {english}
}

@misc{TurnbullBM25Next2015,
  title = {{{BM25 The Next Generation}} of {{Lucene Relevance}}},
  author = {Turnbull, Doug},
  year = {2015},
  month = oct,
  journal = {OpenSource Connections},
  url = {https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/},
  urldate = {2023-04-19},
  abstract = {There's something new cooking in how Lucene scores text. Lucene just switched to something called BM25 in trunk. That means a new scoring formula for Solr and Elasticsearch.},
  langid = {american}
}

@misc{VarunCalculatingDocument2020,
  title = {Calculating {{Document Similarities}} Using {{BERT}} and Other Models},
  author = {Varun},
  year = {2020},
  month = dec,
  journal = {Medium},
  url = {https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630},
  urldate = {2023-02-27},
  abstract = {Making machines identify similar and plagiarised documents},
  langid = {english}
}

@misc{VaswaniAttentionAll2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-04-14},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv}
}

@misc{VigDeconstructingBERT2022,
  title = {Deconstructing {{BERT}}: {{Distilling}} 6 {{Patterns}} from 100 {{Million Parameters}}},
  shorttitle = {Deconstructing {{BERT}}},
  author = {Vig, Jesse},
  year = {2022},
  month = apr,
  journal = {Medium},
  url = {https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77},
  urldate = {2023-04-13},
  abstract = {From BERT's tangled web of attention, some intuitive patterns emerge.},
  langid = {english}
}

@misc{VigDeconstructingBERT2022a,
  title = {Deconstructing {{BERT}}, {{Part}} 2: {{Visualizing}} the {{Inner Workings}} of {{Attention}}},
  shorttitle = {Deconstructing {{BERT}}, {{Part}} 2},
  author = {Vig, Jesse},
  year = {2022},
  month = apr,
  journal = {Medium},
  url = {https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1},
  urldate = {2023-04-13},
  abstract = {A new visualization tool shows how BERT forms its distinctive attention patterns.},
  langid = {english}
}

@inproceedings{VigMultiscaleVisualization2019,
  title = {A {{Multiscale Visualization}} of {{Attention}} in the {{Transformer Model}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Vig, Jesse},
  year = {2019},
  month = jul,
  pages = {37--42},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-3007},
  url = {https://aclanthology.org/P19-3007},
  urldate = {2023-07-28},
  abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.}
}

@techreport{WahleCohesiveDistillation2023,
  title = {A {{Cohesive Distillation Architecture}} for {{Neural Language Models}}},
  author = {Wahle, Jan Philip},
  year = {2023},
  month = feb,
  institution = {{Preprints}},
  doi = {10.22541/au.167528147.79728645/v1},
  url = {https://www.authorea.com/users/580669/articles/621668-a-cohesive-distillation-architecture-for-neural-language-models?commit=2819002ee4589d24c2def104bda90f83d180cdaf},
  urldate = {2023-04-13}
}

@inproceedings{WahleD3Massive2022,
  title = {D3: {{A Massive Dataset}} of {{Scholarly Metadata}} for {{Analyzing}} the {{State}} of {{Computer Science Research}}},
  shorttitle = {D3},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif and Gipp, Bela},
  year = {2022},
  month = jun,
  pages = {2642--2651},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  url = {https://aclanthology.org/2022.lrec-1.283},
  urldate = {2023-03-13},
  abstract = {DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research. We present an initial analysis focused on the volume of computer science research (e.g., number of papers, authors, research activity), trends in topics of interest, and citation patterns. Our findings show that computer science is a growing research field (15\% annually), with an active and collaborative researcher community. While papers in recent years present more bibliographical entries in comparison to previous decades, the average number of citations has been declining. Investigating papers' abstracts reveals that recent topic trends are clearly reflected in D3. Finally, we list further applications of D3 and pose supplemental research questions. The D3 dataset, our findings, and source code are publicly available for research purposes.}
}

@article{WareSTMReport2015,
  title = {The {{STM Report}}:  {{An}} Overview of Scientific and Scholarly Journal Publishing},
  author = {Ware, Mark and Mabe, Michael},
  year = {2015},
  langid = {english}
}

@article{WikipediacontributorsDiscountedCumulative2022,
  title = {Discounted Cumulative Gain},
  author = {{Wikipedia contributors}},
  year = {2022},
  month = aug,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Discounted_cumulative_gain&oldid=1105990923},
  urldate = {2023-03-03},
  abstract = {Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1105990923}
}

@article{WikipediacontributorsMeanReciprocal2022,
  title = {Mean Reciprocal Rank},
  author = {{Wikipedia contributors}},
  year = {2022},
  month = aug,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Mean_reciprocal_rank&oldid=1107032139},
  urldate = {2023-03-03},
  abstract = {The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, 1{$\fracslash$}2 for second place, 1{$\fracslash$}3 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:                                   MRR                  =                                 1                                             |                              Q                                |                                                                    {$\sum$}                        i             =             1                                                  |                          Q                            |                                                                  1                                             rank                                               i                                                         .                          \{\textbackslash displaystyle \{\textbackslash text\{MRR\}\}=\{\textbackslash frac \{1\}\{|Q|\}\}\textbackslash sum \_\{i=1\}\^\{|Q|\}\{\textbackslash frac \{1\}\{\{\textbackslash text\{rank\}\}\_\{i\}\}\}.\textbackslash!\}   where                                                 rank                                   i                                     \{\textbackslash displaystyle \{\textbackslash text\{rank\}\}\_\{i\}\}    refers to the rank position of the first relevant document for the i-th query. The reciprocal value of the mean reciprocal rank corresponds to the harmonic mean of the ranks.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1107032139}
}

@misc{WolfHuggingFaceTransformers2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.03771},
  eprint = {1910.03771},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.03771},
  url = {http://arxiv.org/abs/1910.03771},
  urldate = {2023-03-05},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash url\{https://github.com/huggingface/transformers\}.},
  archiveprefix = {arxiv}
}

@misc{WuGoogleNeural2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  month = oct,
  number = {arXiv:1609.08144},
  eprint = {1609.08144},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.08144},
  url = {http://arxiv.org/abs/1609.08144},
  urldate = {2023-08-03},
  abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  archiveprefix = {arxiv}
}

@misc{XieUnsupervisedData2020,
  title = {Unsupervised {{Data Augmentation}} for {{Consistency Training}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2020},
  month = nov,
  number = {arXiv:1904.12848},
  eprint = {1904.12848},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.12848},
  url = {http://arxiv.org/abs/1904.12848},
  urldate = {2023-08-06},
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  archiveprefix = {arxiv}
}

@inproceedings{YangCARESRankingoriented2009,
  title = {{{CARES}}: A Ranking-Oriented {{CADAL}} Recommender System},
  shorttitle = {{{CARES}}},
  booktitle = {Proceedings of the 9th {{ACM}}/{{IEEE-CS}} Joint Conference on {{Digital}} Libraries},
  author = {Yang, Chenxing and Wei, Baogang and Wu, Jiangqin and Zhang, Yin and Zhang, Liang},
  year = {2009},
  month = jun,
  series = {{{JCDL}} '09},
  pages = {203--212},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1555400.1555432},
  url = {https://doi.org/10.1145/1555400.1555432},
  urldate = {2023-08-16},
  abstract = {A recommender system is useful for a digital library to suggest the books that are likely preferred by a user. Most recommender systems using collaborative filtering approaches leverage the explicit user ratings to make personalized recommendations. However, many users are reluctant to provide explicit ratings, so ratings-oriented recommender systems do not work well. In this paper, we present a recommender system for CADAL digital library, namely CARES, which makes recommendations using a ranking-oriented collaborative filtering approach based on users' access logs, avoiding the problem of the lack of user ratings. Our approach employs mean AP correlation coefficients for computing similarities among users' implicit preference models and a random walk based algorithm for generating a book ranking personalized for the individual. Experimental results on real access logs from the CADAL web site show the effectiveness of our system and the impact of different values of parameters on the recommendation performance.},
  isbn = {978-1-60558-322-8}
}

@misc{YangXLNetGeneralized2020,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  year = {2020},
  month = jan,
  number = {arXiv:1906.08237},
  eprint = {1906.08237},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.08237},
  url = {http://arxiv.org/abs/1906.08237},
  urldate = {2023-09-24},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
  archiveprefix = {arxiv}
}

@inproceedings{ZellersDefendingNeural2019,
  title = {Defending {{Against Neural Fake News}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper_files/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
  urldate = {2023-08-07},
  abstract = {Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.}
}

@misc{ZengKnowledgeTransfer2020,
  title = {Knowledge {{Transfer}} via {{Pre-training}} for {{Recommendation}}: {{A Review}} and {{Prospect}}},
  shorttitle = {Knowledge {{Transfer}} via {{Pre-training}} for {{Recommendation}}},
  author = {Zeng, Zheni and Xiao, Chaojun and Yao, Yuan and Xie, Ruobing and Liu, Zhiyuan and Lin, Fen and Lin, Leyu and Sun, Maosong},
  year = {2020},
  month = sep,
  number = {arXiv:2009.09226},
  eprint = {2009.09226},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.09226},
  urldate = {2023-07-21},
  abstract = {Recommender systems aim to provide item recommendations for users, and are usually faced with data sparsity problem (e.g., cold start) in real-world scenarios. Recently pre-trained models have shown their effectiveness in knowledge transfer between domains and tasks, which can potentially alleviate the data sparsity problem in recommender systems. In this survey, we first provide a review of recommender systems with pre-training. In addition, we show the benefits of pre-training to recommender systems through experiments. Finally, we discuss several promising directions for future research for recommender systems with pre-training.},
  archiveprefix = {arxiv}
}

@article{ZhangLanguageModels2021,
  title = {Language {{Models}} as {{Recommender Systems}}: {{Evaluations}} and {{Limitations}}},
  author = {Zhang, Yuhui and Ding, Hao and Shui, Zeren and Ma, Yifei and Zou, James and Deoras, Anoop and Wang, Hao},
  year = {2021},
  abstract = {Pre-trained language models (PLMs) such as BERT and GPT learn general text representations and encode extensive world knowledge; thus, they can efficiently and accurately adapt to various downstream tasks. In this work, we propose to leverage these powerful PLMs as recommender systems and use prompts to reformulate the session-based recommendation task to a multi-token cloze task. We evaluate the proposed method on a movie recommendation dataset in zero-shot and fine-tuned settings where no or limited training data are available. In the zero-shot setting: we find that PLMs outperform the random recommendation baseline by a large margin; in the meantime, we observe strong linguistic bias when using PLMs as recommenders. In the fine-tuned setting: such bias is reduced with available training data; however, PLMs tend to under-perform traditional recommender system baselines such as GRU4Rec. Our observations demonstrate potential opportunities as well as current challenges in this novel direction.},
  langid = {english}
}

@misc{ZhuAligningBooks2015,
  title = {Aligning {{Books}} and {{Movies}}: {{Towards Story-like Visual Explanations}} by {{Watching Movies}} and {{Reading Books}}},
  shorttitle = {Aligning {{Books}} and {{Movies}}},
  author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year = {2015},
  month = jun,
  number = {arXiv:1506.06724},
  eprint = {1506.06724},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.06724},
  url = {http://arxiv.org/abs/1506.06724},
  urldate = {2023-08-03},
  abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  archiveprefix = {arxiv}
}
